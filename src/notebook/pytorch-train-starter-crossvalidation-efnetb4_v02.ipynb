{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../input/train.csv\n",
      "../../input/test.csv\n",
      "../../input/class_map.csv\n",
      "../../input/.gitkeep\n",
      "../../input/sample_submission.csv\n",
      "../../input/parquet/train_image_data_2.parquet\n",
      "../../input/parquet/test_image_data_2.parquet\n",
      "../../input/parquet/train_image_data_3.parquet\n",
      "../../input/parquet/test_image_data_1.parquet\n",
      "../../input/parquet/test_image_data_0.parquet\n",
      "../../input/parquet/test_image_data_3.parquet\n",
      "../../input/parquet/train_image_data_0.parquet\n",
      "../../input/parquet/train_image_data_1.parquet\n",
      "../../input/feather/train_image_data_2.feather\n",
      "../../input/feather/test_image_data_1.feather\n",
      "../../input/feather/train_image_data_1.feather\n",
      "../../input/feather/test_image_data_3.feather\n",
      "../../input/feather/train_image_data_0.feather\n",
      "../../input/feather/test_image_data_0.feather\n",
      "../../input/feather/test_image_data_2.feather\n",
      "../../input/feather/train_image_data_3.feather\n"
     ]
    }
   ],
   "source": [
    "#+---- Basic Libraries ----+#\n",
    "import sys, os, time, gc, random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from utils import *\n",
    "\n",
    "#+---- Utilities Libraries ----+#\n",
    "#import albumentations as albu\n",
    "import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#+---- Pytorch Libraries ----+#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils import model_zoo\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#+---- List the input data ----+#\n",
    "for dirname, _, filenames in os.walk('../../input/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Initial Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path('../../input')\n",
    "FEATHERDIR = Path('../../input/feather')\n",
    "OUTDIR = Path('../../output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input', '.gitignore', 'README.md', '.git', 'src', 'submission', 'output']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG =False\n",
    "SUBMISSION =False\n",
    "BATCH_SIZE =32\n",
    "NUM_EPOCH = 36\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "MDL_DIR = '../models'\n",
    "LOG_DIR = '../logs'\n",
    "IMAGE_SIZE=128\n",
    "TRAIN_RATIO = 0.9\n",
    "WORKER = 4\n",
    "SEED = 6666\n",
    "MODEL_NAME ='efficientnet-b4'\n",
    "N_Fold = 10\n",
    "CV = True\n",
    "Fold = 1\n",
    "PATIAENCE = 4\n",
    "VER = 'fold_1_mixup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grapheme = 168\n",
    "n_vowel = 11\n",
    "n_consonant = 7\n",
    "n_total = n_grapheme + n_vowel + n_consonant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform class for data preprocessing and augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(datadir, featherdir, data_type='train',\n",
    "                  submission=False, indices=[0, 1, 2, 3]):\n",
    "    assert data_type in ['train', 'test']\n",
    "    if submission:\n",
    "        image_df_list = [pd.read_parquet(datadir / f'{data_type}_image_data_{i}.parquet')\n",
    "                         for i in indices]\n",
    "    else:\n",
    "        image_df_list = [pd.read_feather(featherdir / f'{data_type}_image_data_{i}.feather')\n",
    "                         for i in indices]\n",
    "\n",
    "    print('image_df_list', len(image_df_list))\n",
    "    HEIGHT = 137\n",
    "    WIDTH = 236\n",
    "    images = [df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH) for df in image_df_list]\n",
    "    del image_df_list\n",
    "    gc.collect()\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_char_image(image, threshold=5./255.):\n",
    "    assert image.ndim == 2\n",
    "    is_black = image > threshold\n",
    "\n",
    "    is_black_vertical = np.sum(is_black, axis=0) > 0\n",
    "    is_black_horizontal = np.sum(is_black, axis=1) > 0\n",
    "    left = np.argmax(is_black_horizontal)\n",
    "    right = np.argmax(is_black_horizontal[::-1])\n",
    "    top = np.argmax(is_black_vertical)\n",
    "    bottom = np.argmax(is_black_vertical[::-1])\n",
    "    height, width = image.shape\n",
    "    cropped_image = image[left:height - right, top:width - bottom]\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.ColorJitter(0.5,0.5,0.5,0.5),\n",
    "        transforms.RandomAffine(degrees=0.6),\n",
    "        transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    \n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliAIDataset(Dataset):\n",
    "    def __init__(self, images, labels=None, transform=None, indices=None):\n",
    "        self.transform = transform\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        if indices is None:\n",
    "            indices = np.arange(len(images))\n",
    "        self.indices = indices\n",
    "        self.train = labels is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"return length of this dataset\"\"\"\n",
    "        return len(self.indices)\n",
    "      \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Return i-th data\"\"\"\n",
    "        i = self.indices[i]\n",
    "        x = self.images[i]\n",
    "        # Opposite white and black: background will be white and\n",
    "        # for future Affine transformation\n",
    "        x = (255 - x).astype(np.float32) #/ 255.\n",
    "        x = crop_char_image(x)\n",
    "        x = Image.fromarray(x).convert(\"RGB\")\n",
    "        x = self.transform(x)\n",
    "        if self.train:\n",
    "            y = self.labels[i]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "Fold 10\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATADIR/'train.csv')\n",
    "train['id'] = train['image_id'].apply(lambda x: int(x.split('_')[1]))\n",
    "X, y = train[['id', 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic']]\\\n",
    ".values[:,0], train.values[:,1:]\n",
    "train['fold'] = np.nan\n",
    "mskf = MultilabelStratifiedKFold(n_splits=N_Fold)\n",
    "for i, (_, index) in enumerate(mskf.split(X, y)):\n",
    "    print('Fold '+str(i+1))\n",
    "    train.iloc[index, -1] = i\n",
    "train['fold'] = train['fold'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_df_list 4\n",
      "CPU times: user 11.3 s, sys: 13.9 s, total: 25.3 s\n",
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train = pd.read_csv(DATADIR/'train.csv')\n",
    "train_labels = train[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].values\n",
    "indices = [0] if DEBUG else [0, 1, 2, 3]\n",
    "train_images = prepare_image(\n",
    "    DATADIR, FEATHERDIR, data_type='train', submission=False, indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dataset = len(train_images)\n",
    "\n",
    "if not CV:\n",
    "    train_data_size = 200 if DEBUG else int(n_dataset * TRAIN_RATIO)\n",
    "    valid_data_size = 100 if DEBUG else int(n_dataset - train_data_size)\n",
    "    perm = np.random.RandomState(777).permutation(n_dataset)\n",
    "    print('perm', perm)\n",
    "\n",
    "    train_dataset = BengaliAIDataset(\n",
    "        train_images, train_labels, transform=data_transforms['train'],\n",
    "        indices=perm[:train_data_size])\n",
    "\n",
    "    valid_dataset = BengaliAIDataset(\n",
    "        train_images, train_labels, transform=data_transforms['val'],\n",
    "        indices=perm[train_data_size:train_data_size+valid_data_size])\n",
    "else:\n",
    "    valid_idx = np.array(train[train['fold']==Fold].index)\n",
    "    trn_idx = np.array(train[train['fold']!=Fold].index)\n",
    "    trn_idx = trn_idx[:200] if DEBUG else trn_idx\n",
    "    valid_idx = valid_idx[:100] if DEBUG else valid_idx\n",
    "    \n",
    "    train_dataset = BengaliAIDataset(\n",
    "        train_images, train_labels, transform=data_transforms['train'],\n",
    "        indices=trn_idx)\n",
    "    valid_dataset = BengaliAIDataset(\n",
    "        train_images, train_labels, transform=data_transforms['val'],\n",
    "        indices=valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKER)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKER)\n",
    "\n",
    "dataloaders = {'train':train_loader, 'val': valid_loader}\n",
    "dataset_sizes = {'train':len(train_dataset), 'val': len(valid_dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image torch.Size([3, 128, 128]) label [159   0   0]\n"
     ]
    }
   ],
   "source": [
    "image, label = train_dataset[1]\n",
    "print('image', image.shape, 'label', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model/Train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohem_loss(rate, cls_pred, cls_target):\n",
    "    batch_size = cls_pred.size(0) \n",
    "    ohem_cls_loss = F.cross_entropy(cls_pred, cls_target, reduction='none', ignore_index=-1)\n",
    "\n",
    "    sorted_ohem_loss, idx = torch.sort(ohem_cls_loss, descending=True)\n",
    "    keep_num = min(sorted_ohem_loss.size()[0], int(batch_size*rate) )\n",
    "    if keep_num < sorted_ohem_loss.size()[0]:\n",
    "        keep_idx_cuda = idx[:keep_num]\n",
    "        ohem_cls_loss = ohem_cls_loss[keep_idx_cuda]\n",
    "    cls_loss = ohem_cls_loss.sum() / keep_num\n",
    "    return cls_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_recall(pred_labels, y, n_grapheme=168, n_vowel=11, n_consonant=7):\n",
    "    recall_grapheme = sklearn.metrics.recall_score(pred_labels[0], y[0], average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(pred_labels[1], y[1], average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(pred_labels[2], y[2], average='macro')\n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_score = np.average(scores, weights=[2, 1, 1])\n",
    "    print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, '\n",
    "           f'total {final_score}')\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(preds_list, label_list):\n",
    "    #preds_list is torch tensor to device\n",
    "    #label_list is torch tensor to device\n",
    "    _, pred0 = torch.max(preds_list[0], 1)\n",
    "    _, pred1 = torch.max(preds_list[1], 1)\n",
    "    _, pred2 = torch.max(preds_list[2], 1)\n",
    "    p0 = pred0.cpu().numpy()\n",
    "    p1 = pred1.cpu().numpy()\n",
    "    p2 = pred2.cpu().numpy()\n",
    "    pred_labels = [p0, p1, p2]\n",
    "    #print(pred_labels)\n",
    "    a0 = label_list[0].cpu().numpy()\n",
    "    a1 = label_list[1].cpu().numpy()\n",
    "    a2 = label_list[2].cpu().numpy() \n",
    "    y = [a0, a1, a2]\n",
    "    #print(y)\n",
    "    return pred_labels, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "def cutmix(data, targets1, targets2, targets3, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets1 = targets1[indices]\n",
    "    shuffled_targets2 = targets2[indices]\n",
    "    shuffled_targets3 = targets3[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "\n",
    "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, \\\n",
    "               shuffled_targets3, lam]\n",
    "    return data, targets\n",
    "\n",
    "def mixup(data, targets1, targets2, targets3, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets1 = targets1[indices]\n",
    "    shuffled_targets2 = targets2[indices]\n",
    "    shuffled_targets3 = targets3[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3,\\\n",
    "               shuffled_targets3, lam]\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def cutmix_criterion(preds1,preds2,preds3, targets):\n",
    "    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1],\\\n",
    "    targets[2], targets[3], targets[4], targets[5], targets[6]\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2) +\\\n",
    "lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4) +\\\n",
    "lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)\n",
    "\n",
    "def mixup_criterion(preds1,preds2,preds3, targets):\n",
    "    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], \\\n",
    "    targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2)\\\n",
    "+ lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4) +\\\n",
    "lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixuplist = random.sample( range(length), int(0.2*length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler,start_epoch,\n",
    "                num_epochs, device, patiance):\n",
    "    since = time.time()\n",
    "    \n",
    "    trn_loss_list =[]\n",
    "    trn_acc_list = []\n",
    "    val_loss_list =[]\n",
    "    val_acc_list = []\n",
    "    epoch_list = []\n",
    "    recall_list = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs)[start_epoch:]:\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        mix_cnt = 0\n",
    "        print(mix_cnt)\n",
    "        \n",
    "        \n",
    "        if early_stopping_counter == patiance:\n",
    "            print(f'Early Stopped since loss have not decreased for {patiance} epoch.')\n",
    "            break\n",
    "        epoch_list.append(epoch)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            dataset_sizes = len(dataloaders[phase].dataset)\n",
    "            length = int(np.floor(dataset_sizes/BATCH_SIZE))\n",
    "            mixuplist = random.sample( range(length), int(0.2*length))\n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (inputs, labels) in enumerate(tqdm(dataloaders[phase])):\n",
    "                #print(inputs.shape)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.transpose(1,0).to(device) #use when single label for one image\n",
    "\n",
    "                grapheme_root = labels[0]\n",
    "                vowel_diacritic = labels[1]\n",
    "                consonant_diacritic = labels[2]\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if idx in mixuplist:\n",
    "                        mix_cnt += 1\n",
    "                        inputs_mixed, labels_mixed = mixup(inputs, grapheme_root, \\\n",
    "                                                           vowel_diacritic, consonant_diacritic, 0.1)\n",
    "                    \n",
    "                        outputs = model(inputs_mixed) \n",
    "                        grapheme_root_prd = outputs[0]\n",
    "                        vowel_diacritic_prd = outputs[1]\n",
    "                        consonant_diacritic_prd = outputs[2]\n",
    "                        loss = mixup_criterion(grapheme_root_prd,vowel_diacritic_prd,\\\n",
    "                                           consonant_diacritic_prd, labels_mixed)\n",
    "                        \n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        grapheme_root_prd = outputs[0]\n",
    "                        vowel_diacritic_prd = outputs[1]\n",
    "                        consonant_diacritic_prd = outputs[2]\n",
    "                        loss = (1/4)*(2*criterion(grapheme_root_prd, grapheme_root)+\\\n",
    "                              criterion(vowel_diacritic_prd, vowel_diacritic) +\\\n",
    "                                 criterion(consonant_diacritic_prd, consonant_diacritic))\n",
    "                                   \n",
    "\n",
    "                    #loss = criterion(outputs, labels)\n",
    "                    #output shape : (batch size, class number)\n",
    "                    #label shape : batch size\n",
    "                    \n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics: inputs.size(0) is batch size\n",
    "                epoch_loss += loss.item() * inputs.size(0) # total loss for this batch\n",
    "                epoch_corrects += torch.sum(torch.max(outputs[0], 1)[1] == labels[0])+\\\n",
    "                    torch.sum(torch.max(outputs[1], 1)[1] == labels[1])+\\\n",
    "                    torch.sum(torch.max(outputs[2], 1)[1] == labels[2])\n",
    "            if phase == 'val':\n",
    "                #scheduler.step()\n",
    "                # if plateau scheduler use following\n",
    "                scheduler.step(epoch_loss)\n",
    "                pred, lbls = get_pred(outputs, labels)\n",
    "                recall = macro_recall(pred, lbls, \\\n",
    "                                      n_grapheme=168, n_vowel=11, n_consonant=7)\n",
    "                print('valid recall score is {:.3f}'.format(recall))\n",
    "\n",
    "            epoch_loss = epoch_loss / dataset_sizes\n",
    "            epoch_acc = epoch_corrects.double() / (dataset_sizes*3)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                trn_loss_list.append(epoch_loss)\n",
    "                trn_acc_list.append(epoch_acc.cpu().numpy())\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if not os.path.exists(f'{MDL_DIR}/{MODEL_NAME}_{VER}'):\n",
    "                    os.mkdir(f'{MDL_DIR}/{MODEL_NAME}_{VER}')\n",
    "                save_path = f'{MDL_DIR}/{MODEL_NAME}_{VER}/{MODEL_NAME}_'+str(epoch)+'.pth'\n",
    "                torch.save(model_ft.state_dict(),save_path)\n",
    "                best_epoch = epoch\n",
    "            \n",
    "            if phase == 'val':\n",
    "                val_loss_list.append(epoch_loss)\n",
    "                val_acc_list.append(epoch_acc.cpu().numpy())\n",
    "                recall_list.append(recall)\n",
    "                # Early Stopping\n",
    "                if epoch == 0 or epoch == start_epoch:\n",
    "                    best_loss = epoch_loss\n",
    "                else:\n",
    "                    if epoch_loss < best_loss:\n",
    "                        best_loss = epoch_loss\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "                        print(f'Early stopping counter: {early_stopping_counter}')\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    #if not os.path.exists(f'{LOG_DIR}/log_{MODEL_NAME}_{VER}.csv'):\n",
    "    log = pd.DataFrame()\n",
    "    log['Epoch'] = epoch_list\n",
    "    log['Train Loss'] = trn_loss_list\n",
    "    log['Train Acc'] = trn_acc_list\n",
    "    log['Valid Loss'] = val_loss_list\n",
    "    log['Valid Acc'] = val_acc_list\n",
    "    log['Recall'] = recall_list\n",
    "    log.to_csv(f'{LOG_DIR}/log_{MODEL_NAME}_{VER}.csv',index=False)\n",
    "        \n",
    "    return model, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class bengali_model(nn.Module):\n",
    "    def __init__(self, num_classes1, num_classes2, num_classes3):\n",
    "        super(bengali_model, self).__init__()\n",
    "        #pretrain models\n",
    "        #self.model = pretrainedmodels.__dict__[MODEL_NAME](pretrained=None)\n",
    "        #num_ftrs = self.model.last_linear.in_features\n",
    "        #self.model.last_linear = nn.Identity()\n",
    "        \n",
    "        # EfficientNet\n",
    "        self.model = EfficientNet.from_pretrained(MODEL_NAME)\n",
    "        num_ftrs = 1792\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_ftrs, num_classes1)\n",
    "        self.fc2 = nn.Linear(num_ftrs, num_classes2)\n",
    "        self.fc3 = nn.Linear(num_ftrs, num_classes3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.model(x) #pretrain models\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = self.model.extract_features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        out1 = self.fc1(x)\n",
    "        out2 = self.fc2(x)\n",
    "        out3 = self.fc3(x)\n",
    "        return out1, out2, out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method EfficientNet.extract_features of EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        48, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        12, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (16): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (17): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (18): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (19): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (20): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (21): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (22): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (23): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (24): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (25): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (26): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (27): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (28): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (29): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (30): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (31): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2688, 112, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        112, 2688, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (_fc): Linear(in_features=1792, out_features=1000, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EfficientNet.from_pretrained(MODEL_NAME).extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "# --- Model --- Stage 1\n",
    "\n",
    "model_ft = bengali_model(n_grapheme, n_vowel, n_consonant)\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.7, patience=5, min_lr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5268"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "----------\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4535ac116b994888926b333403dc2bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 1.7092 Acc: 0.7616\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7cf0f9617d4362bd6036d90611b408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.625, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.7669871794871794\n",
      "valid recall score is 0.767\n",
      "val Loss: 1.5056 Acc: 0.8222\n",
      "Epoch 2/36\n",
      "----------\n",
      "1908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryoooo1114/anaconda3/envs/analysis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c59f0f9cf4e47e1a4b0285e759075ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 1.1781 Acc: 0.8331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182885b756c4442690dd653c309dada7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.0, vowel 0.2375, consonant 0.2230769230769231, total 0.11514423076923078\n",
      "valid recall score is 0.115\n",
      "val Loss: 1.2562 Acc: 0.8276\n",
      "Epoch 3/36\n",
      "----------\n",
      "3771\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53d8d906aed4f8a8148d45e0a5ead19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 1.0625 Acc: 0.8431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbc0bdf15814143accf80f78f5c1375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.7727272727272727, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.8408508158508159\n",
      "valid recall score is 0.841\n",
      "val Loss: 1.1764 Acc: 0.8476\n",
      "Epoch 4/36\n",
      "----------\n",
      "5623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91a79ff03bb4ee284c5b381924f248f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.9701 Acc: 0.8550\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a344fa2d2a5447208b3fd09d2a99d792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.7857142857142857, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.8473443223443223\n",
      "valid recall score is 0.847\n",
      "val Loss: 1.0781 Acc: 0.8471\n",
      "Epoch 5/36\n",
      "----------\n",
      "7470\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf6ec2db6934f80a9e28f4a72e9d384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.9318 Acc: 0.8608\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06a4b8365544212bf6ee1d513a26add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.8571428571428571, vowel 0.8333333333333333, consonant 1.0, total 0.8869047619047619\n",
      "valid recall score is 0.887\n",
      "val Loss: 1.2085 Acc: 0.8485\n",
      "Early stopping counter: 1\n",
      "Epoch 6/36\n",
      "----------\n",
      "9353\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c93cf5174da43a0a9187863b95312fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.8913 Acc: 0.8645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7c89f275da419c89b751caffb662a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.045454545454545456, vowel 0.23333333333333334, consonant 0.25, total 0.14356060606060606\n",
      "valid recall score is 0.144\n",
      "val Loss: 1.1951 Acc: 0.8482\n",
      "Early stopping counter: 2\n",
      "Epoch 7/36\n",
      "----------\n",
      "11248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23aa8790252e4e30b9455b8626fe2c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.8670 Acc: 0.8637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7fb53f4c9247d28db74dec6aca42fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.7727272727272727, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.8408508158508159\n",
      "valid recall score is 0.841\n",
      "val Loss: 1.1998 Acc: 0.8593\n",
      "Early stopping counter: 3\n",
      "Epoch 8/36\n",
      "----------\n",
      "13137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c642187a9c4b308b10470629ceb0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.8408 Acc: 0.8664\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a83d6ecd2444c49499923e2a242c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.8571428571428571, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.883058608058608\n",
      "valid recall score is 0.883\n",
      "val Loss: 1.1301 Acc: 0.8666\n",
      "Early stopping counter: 4\n",
      "Epoch 9/36\n",
      "----------\n",
      "14988\n",
      "Early Stopped since loss have not decreased for 4 epoch.\n",
      "Training complete in 413m 46s\n",
      "Best val Acc: 0.866577\n"
     ]
    }
   ],
   "source": [
    "# fold 1 Best val Acc: 0.974806\n",
    "# fold 2 Best val Acc: 0.975171 \n",
    "# with mixup fold 1\n",
    "model_ft, best_epoch = train_model(model_ft, dataloaders, criterion, optimizer, scheduler,0, NUM_EPOCH, DEVICE, PATIAENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd5iU9dn28e+1LLDSIaCiKCAiapSorBC7Yu8aNQKuJsaIxBLbE31M8TGWRM1rYonG2EUUjd0Yu4k1RFykCKLSEcWADVBRyl7vH9esu+DOMiw7e8/sfX6OY44t98zsBSxzzq+buyMiIulVknQBIiKSLAWBiEjKKQhERFJOQSAiknIKAhGRlCtNuoC11bVrV+/Vq1fSZYiIFJVx48Z95O7d6rpWdEHQq1cvKisrky5DRKSomNmcbNfUNSQiknIKAhGRlFMQiIiknIJARCTlFAQiIimnIBARSTkFgYhIyqUnCKZOhbPPhmXLkq5ERKSgpCcIZs2Cq6+Gp55KuhIRkYKSniDYd1/o1g1GjUq6EhGRgpKeIGjZEoYMgcceg88+S7oaEZGCkZ4gAKiogK+/hgcfTLoSEZGCka4g2HFH6NtX3UMiIrWkKwjMolXwwgswd27S1YiIFIR0BQHAccfFx9Gjk61DRKRApC8I+vSBnXaCu+4C96SrERFJXPqCAKJ7aMoUmDQp6UpERBKXziD44Q+htFSDxiIipDUIunaFAw+Ee+6BlSuTrkZEJFHpDAKI7qEPPogZRCIiKZbeIDj0UOjQQd1DIpJ66Q2C9daDo4+OVcZffpl0NSIiiUlvEEB0Dy1ZEvsPiYikVLqDYI89oEcPdQ+JSKqlOwhKSmDYsDijYOHCpKsREUlEuoMAonto5Uq4776kKxERSYSCYNttoX9/dQ+JSGopCCBaBa+9BtOmJV2JiEiTUxAADB0aW1TffXfSlYiINDkFAcTMob32iu4h7UgqIimTtyAws9vMbIGZTa7nPnua2QQzm2JmL+arlpxUVMCMGdFFJCKSIvlsEdwBHJDtopl1Am4ADnP37wLH5LGWNTvqKCgr06CxiKRO3oLA3V8CPqnnLsOAh9x9bub+C/JVS046dIDDD4d774XlyxMtRUSkKSU5RrAF0NnMXjCzcWZ2QrY7mtlwM6s0s8qF+Vz4VVEBH38MTz+dv58hIlJgkgyCUmAAcDCwP/AbM9uirju6+03uXu7u5d26dctfRfvvD9/5ThxjKSKSEkkGwTzgaXf/wt0/Al4CvpdgPdCyJQwZEpvQLVqUaCkiIk0lySB4FNjVzErNrA0wCJiaYD2hogK++goeeijpSkREmkQ+p4+OBsYA/cxsnpmdZGYjzGwEgLtPBZ4CJgFjgVvcPetU0yYzaBD06aPZQyKSGqX5emJ3H5rDff4A/CFfNTSIWbQKLr4Y5s2LxWYiIs2YVhbX5bjjYoXx6NFJVyIikncKgrr07RtdROoeEpEUUBBkU1EBkybFTUSkGVMQZHPssdCihXYkFZFmT0GQTbducMABEQRVVUlXIyKSNwqC+hx/PLz/PryY7MaoIiL5pCCoz6GHQvv2GjQWkWZNQVCfNm1ie+oHHoClS5OuRkQkLxQEa1JRAYsXw9//nnQlIiJ5oSBYkz33hI02UveQiDRbCoI1adEChg2DJ5+Ejz5KuhoRkUanIMhFRQWsWAF/+1vSlYiINDoFQS7694dttlH3kIg0SwqCXFTvSDpmDMyYkXQ1IiKNKjVB4B6v4w02bFh81JYTItLMpCYIbrsNdt55HWaBbrJJzCAaNSpSRUSkmUhNEBx3HGy/PZxwAsya1cAnqaiAadPg9dcbtTYRkSSlJgjKymKBsDscfXQcS7zWjj4aWrfWoLGINCupCQKAzTaDkSPhjTfg7LMb8AQdO8Jhh8G998Ly5Y1en4hIElIVBBCv4+edBzfe2MA39hUVsHAhPPNMo9cmIpKE1AUBwGWXwe67wymnwJQpa/ngAw6ALl3UPSQizUYqg6C0NHp32rePzUWXLFmLB7dqFaeXPfJIbEYnIlLkUhkEAN27w+jRMQlo+PC1nBFaURGjzQ8/nLf6RESaSmqDAGCvveDSS6N1cP31a/HAnXaC3r3VPSQizUKqgwDg/PPhkEPgnHPgtddyfFD1lhPPPw8ffJDX+kRE8i31QVBSAnfeCRtvDMccAx9/nOMDjzsu+pNGj85rfSIi+Zb6IICYBHT//fDf/8Yb/aqqHB7Urx/suKO6h0Sk6CkIMsrL4Zpr4Kmn4He/y/FBFRUwYQJMnpzX2kRE8klBUMspp0SPz4UXwnPP5fCAIUPiBDPtSCoiRUxBUItZrDjeaqvYdfr999fwgPXXh/33jyDIqT9JRKTwKAhW065dbE735ZexbmyNWwpVVMB778HLLzdJfSIijU1BUIettoJbboFXX4ULLljDnQ8/PNLjrruapDYRkcamIMhiyBA47TS46ip46KF67timDfzgBzHtqEF7W4uIJEtBUI+rrooZoieeCNOn13PHiorYd+jxx5usNhGRxqIgqEfr1vFGv7Q0zqRZujTLHQcPhg031JoCESlKCoI16NkzXt8nToQzzshypxYtYprRE0+sxdJkEZHCkLcgMLPbzGyBmdW72srMdjSzFWZ2dL5qWVcHHgi//jXceivcfnuWO1VUxBSj++9v0tpERNZVPlsEdwAH1HcHM2sBXAEU/HFfF10UPUCnnhqtg2/ZbjvYemt1D4lI0clbELj7S8Ana7jbGcCDwIJ81dFYWrSI/eW6dInxgkWLVrtD9Y6kr74KM2cmUqOISEMkNkZgZhsDRwJ/SaqGtbX++nDffTBrFvzkJ3UcZjNsWHy8554mr01EpKGSHCy+Gjjf3de4N4OZDTezSjOrXLhwYROUlt2uu8IVV8TagquvXu1iz55xGPKoUWt55JmISHKSDIJy4F4zmw0cDdxgZkfUdUd3v8ndy929vFu3bk1ZY53OOQeOOALOOy96glZx/PHwzjswblwitYmIrK3EgsDde7t7L3fvBTwAnOrujyRVz9owi9lDPXvGfkQLao9wHH10HHCvQWMRKRL5nD46GhgD9DOzeWZ2kpmNMLMR+fqZTalTp9ic7qOPYuvqlStrXTj00BhZXrEi0RpFRHKxVkFgoW0u93X3oe7e3d1bunsPd7/V3W909xvruO+P3f2BtamlEGy3XRx6/9xzcPHFtS5UVEQz4dlnE6tNRCRXawwCMxtpZh3MrA3wJjDdzM7Jf2nF4aSTYi+iSy6J082AWIHWubO6h0SkKOTSIujv7ouBI4BngZ7Aj/NZVLH5859h222jITB3LrFJ0Q9/CA8/DEuWJF2eiEi9cgmClmZWChwOPOruywAdx1VLmzYxXrBsWbz+L1tGpMLSpfBIUYx/i0iK5RIEtwBzgc7Ai2a2KfB5XqsqQn37xkyi116DX/wC2Hln6NVL3UMiUvDWGATu/id338jd93N3B94DBue/tOJz1FFw9tlw7bXwtwdKYjrRc8/B/PlJlyYiklUug8Wnm1mHzOd/BV4Ddst3YcXqiiuiMXDSSTBzp+PiUPt77026LBGRrHLpGhru7ovNbD9gA+Bk4Mr8llW8WraM/YjKyuCw87di5fYD1D0kIgUtlyCo3jTnIOAud5+Y4+NSq0eP2HfurbdgdEkFvPFGfCEiUoByeUGfaGZPAIcAT5pZO2rCQbLYd984w+DccUOpKmkBd9+ddEkiInXKJQhOBC4CBrr7l0AZcFI+i2oufv1r2H7/DXjW9+Xr2++O8QIRkQKTy6yhlUBX4DwzuxzY0d3H572yZqCkJIYH/tG5gtbz57DkyVeSLklE5FtymTV0GXAeMDNz+4WZXZrvwpqLrl2h4oEj+Jy2/PvUUWoUiEjByaVr6FBgn8yZADcB+wGH5bes5mXgXm2ZV34kg+b+jT/9/qukyxERWUWus3/aZ/lcctTvkgo6sYgxFz7Biy8mXY2ISI1cguBK4A0zu8XMbgUqgcvzW1bzY/vsTdX6GzC8zSiGDIEPP0y6IhGRkMtg8ShgV+AJ4B/A7sBzea6r+SktpWTYUPb5+h+UfPYJQ4fq3BoRKQw5dQ25+/vu/lDm9j7RKpC1VVFByfJlPDj0AV54AS68MOmCREQavkLYGrWKtNhhB9hyS74/fRTDh8Pvfw+PP550USKSdg0NAq0sbgizOKfg5Ze59pzZbL89HH88zJqVdGEikmal2S6Y2Z+o+wXfgI55q6i5O+44+PWvaf3gPTzwwC/ZYQc45hh45ZXYqE5EpKnV1yKYDEyp4zYZ0JnFDdWrF+y2G9x1F5v1dkaOhHHj4hwDEZEkZG0RuPutTVlIqlRUwCmnwPjxHHbYDpx/fpxjsMsucUlEpClpO+kkHHMMtGoFd90FwKWXwu67RzZMmZJwbSKSOgqCJHTuDAcfDKNHw4oVlJbGIWbt28dxl0uWJF2giKSJgiApFRXw3//C888D0L17hMG0aTB8OLjmZYlIE8ll99GuZnaemd1gZjdV35qiuGbtoIOgU6dVjrHcc0+47LIIhBtuSK40EUmXXFoEjxJnFb8CPF/rJuuirCzGCh56CD7//Jtvn3ceHHJIzCJ6Tht5iEgTyCUI2rr7ue5+j7vfV33Le2VpUFEBX34Jjz76zbdKSmDkSNhkkzjucuBAuPNO+Eq7V4tInuQSBE+a2X55rySNdt0VNt10le4hiLHk8ePhuuti4PjHP4YePeD887UKWUQaXy5BMAJ4ysw+N7NPzOxTM/sk34WlQklJrDR+5pkYOK6lQwc4/XR4660YT95jD7jqKujTBw49FJ56Skcgi0jjyCUIugItiW0lumW+7pbPolKloiJe0e+9t87LZjB4MDz4IMyeDb/6FYwdCwceCP36wR//CJ9+2rQli0jzYp5lnqKZ9XX3aWbWv67r7j4pr5VlUV5e7pWVzWwX7B12gBYt4PXXc7r7smURDNdfD6++CuutB8OGwWmnwfbb57lWESlKZjbO3cvrulZfi+B/Mx+vr+P250atMO2OPx4qK+Htt3O6e6tWMHRobFQ3fnw0KkaPjjzZeWe4+274+us81ywizUbWIHD3kzIfd6vjtnvTlZgCQ4bEeMHdd6/1Q7fbDm66Cd5/H/70J/joowiGTTaJbqS5c/NQr4g0K1m7hla5k9mWwNbANxslu/s9eawrq2bZNQSw//7w7rswY0aEQgNVVcX6g+uvrzn05rDDotto771jzEFE0qehXUPVD/41cBNwI3AgcDVwdKNWKPE2fvZs+Pe/1+lpSkpgv/1iacLMmbFA7ZVXYk3CVlvBtdfCokWNU7KINA+5vPU8FtgLmO/uxwPfA9rmtao0OvJIaNPmW2sK1kXPnnEc5nvvxSK1Tp3gzDNh441hxAh4881G+1EiUsTW2DVkZmPdfaCZjQP2BD4Hprr7lmt43G3AIcACd9+mjuvHAecTJ54tAX7m7hPXVHCz7RqCWFPw5JMwfz60bp2XHzFuXHQbjR4dq5V32y26jY48MgahpbC8/Tbcdx+0bQtdutR908l2kov6uoayHkxTy3gz6wTcBlQCi4GxOTzuDmJ20cgs12cBe7j7p2Z2INH9NCiH522+KirgnnsiDI44Ii8/YsAAuO02+MMf4Pbb4S9/ibHqDTeMXU+HD48WgySrqiq68S64YM3bi6y3Xt0B0blz9vDo0gXatdOYkYR6WwRmZsCG7j4/8/XmQAd3fyOnJzfrBTxeV4tgtft1Bia7+xpfgpp1i2DFCthoozil5oEHmuRHVlXB009HK+GJJ2KM4cgjo5Wwxx56oUjCrFlw4onw4ouxAeFNN8WL9iefxOLBTz7J/bZ0afafU1qaPSTqC5GOHWPZS1Nxh5UrY0r0smU1t8b4um3bOANk662b7s+TlPpaBLl0DU1e0wt5PY/tRW5B8D/Alu7+0yzXhwPDATbddNMBc+bMaUg5xeHMM+HGG2PLiU6dmvRHz5wZP/rWW+NFZOut4dRT4YQT4tAcyS93uOUWOOecCOBrrol9ptYljJcurQmPtQmRxYuzP6dZ/GpmC5CSksZ9sV62LD/nc7RqBcuXx3MPGBDLeYYOhfXXb/yfVQjWNQhGAVe5+/gG/OBerCEIzGwv4AZgV3f/eE3P2axbBBCriwcOhJtvhp/WmYt5t3Rp9Etff32sc2vXLsLgtNPS8c4pCR98ACefHK2yvfaKbruePZOrZ/ly+OyztQ+QTz+NVmarVjW31q0L7+vS0gi0//43xsvuugveeCNaOgccEKFw2GHR7dZc1BcEuHudN6A083EKsAJ4B3gDGA+8ke1xqz1HL6LLJ9v1/sAMYItcns/dGTBggDdrVVXuW2zhvsceSVfi7u6vveZ+wgnurVu7g/uee7rff7/7smVJV9Y8VFW5jx7t3rmz+3rruV97rfvKlUlX1XBVVXErRpMnu59/vvvGG8fveocO7j/5ifsLLxT3v0k1oNKzvRZnvZB5sQf61HXL9rjVniNrEACbAtOBnXN5rupbsw8Cd/eLL45/mjlzkq7kGwsXul9+uXvPnlHaRhu5//a37vPnJ11Z8Vq40P2YY+Lvc9Ag93feSboicXdfscL9uefcf/Qj97Zt49+nZ0/3X/3K/e23k66u4RoaBOOzXcvlBowG5gPLgXnAScSW1iMy128BPgUmZG5Zi6x9S0UQzJgR/zRnnVVwb0VWrHB/7DH3/fePEktL3Y891v2VV5KurLg89pj7Bhu4t2zp/rvfuS9fnnRFUpfPP3cfNSp+30tK4nd+xx3dr7sugryY1PcaW9/uo/OAP2brb3L3rNfyqdmPEVQbOjS2pv7+9+MA4wLcVnTatJh+evvt0Z88eDBccklsfCd1W7wYzjor/s7694++6f517u8rhWb+/JjdfdddMHFijDMceGCMnx1ySOGv52joFhMtgHZA+yw3yad77onlwDNnQnk5nHFGvNoWkL594zyE99+Hq6+GKVNgl13goINi4Zqs6p//hG23jaNHf/nLmBegECge3bvDuefChAkRBGedFZMpjjmmZh3OK6/kZ4ZT3mVrKpDjgHBT31LRNVTbp5+6n3ZatEs32MB95MiCHY37/HP3K65w79IlmtBHHOE+aVLSVSXviy/czzgj/k622MJ9zJikK5LGsmKF+zPPuFdUuLdpE//GvXu7/+Y37u++m3R1qyKJMYJ83VIXBNXGjYsRRXDffXf3N99MuqKsFi2K8e4OHdzNYgxh6tSkq0rGmDHuffvGP9vPfx6hIM3TkiXxPm2ffeL3Hty//3336693/+ijpKtreBB0yXYtyVtqg8A9Bo5vvjnecrdo4X7uue6LFyddVVaffBIzLdq2jQbNCSfEOHgafPWV+wUXxJ97003dn38+6YqkKc2b537lle7bbBOvsi1bRgv5wQfjdyMJDQqCQr2lOgiqLVzo/tOf+jfzOO+7r2C7i9zdFyyIzCori1lGw4e7z52bdFX5M2GCe//+8c/zk59EC0nSqarKffx493POiZ5diDUjI0a4v/pq0/63VRA0V2PGuG+/ffwz7rtvwU9y/uAD99NPd2/VKm5nnBHfay6WL3e/7LJ497fBBjFFVKTa8uXuTz7pPmxYLB4E9z593C+6yH369Pz/fAVBc7ZiRUxq7tgxXoF++cuC74ieM8f95JOjd2u99dz/53+i1VDM3n67Zgjnhz8sjD5hKVyLFrnffrv74ME14wm77OJ+443RpZoPCoI0+PBD9+OP92+WQT7ySEF3F7m7T5sWJZeUuLdrF+MJ+fpPkC8rV7pfc00EWpcu7vfem3RFUmzmznX//e/dt946/vu2auV+1FHxX/jrrxvv5ygI0uTFF2tGqA4+uChGZ996K95FQzRsLr64OPrVZ89232uvqPugg5pXN5c0vaoq98pK9zPPdF9//fi9+s53Yvb4f/6z7u/rFARps2yZ+//7f/E2u6wsNgVaujTpqtZowgT3ww+v+Q9w5ZWF2ctVVeV+663u7dvHX/HNNxd840uKzLJl7o8/HlOvy8r8mzUod97Z8OesLwhyObNYik3LlrEE8u23Yy/d//u/WNL61FNJV1av730PHnkExo6FHXeE886DzTaLk7rWdEpXU5k/P/5KTzoJdtghzn3+6U91gI80rpYt4eCDY5eZDz+Mcyq6d4cvvsjTD8yWEIV6U4ugAZ59Nt5OgPsPflA0czdfeSW2vQb3Hj3c//rXZLe/vu++GAcoK3P/058Kbj9ASYF1aXmiFkHK7bMPTJoEl10W5yFvuSVccUUc/VTAdtkF/vUveP552GQTOOUU6Ncv9upZsaLp6vj449gD8NhjYfPNYfz42GemRP97pInlq+WpX+W0aN06djp76y3Ybz/43/+F7baLV9oCN3gwvPpqnN7VuXMc3/jd70azuaoqvz/7H/+AbbaJI6QvvTTq2HLL/P5MkaamIEibXr3g4Yfh8cej433wYBg2LDq/C5hZbPlbWQkPPRRHDg4dGuMKDz/c+Ds+Ll4cff+HHAJdu8ZOob/6VWw9LNLcKAjS6uCDY9/oCy+MV9Z+/WIv6absc2kAMzjyyNgGePTo6N36wQ9icPmJJxonEF54IbaHvv12OP/8CJ/ttlv35xUpVAqCNFtvPfjtb2Hy5OiQP/tsGDAg+j8KXEkJDBkSWXbHHXFw+sEHxx/jn/9s2HMuXRp9/3vtFbM2Xn4ZLr88etVEmjMFgcQI6BNPwIMPwqefwq67woknwoIFSVe2RqWl8KMfxUzZG2+E996DvfeOF/NXXsn9ecaOjUPgrrkGTj89Dh/RSWuSFgoCCWbRxzJ1avSHjBoV3UV/+QusXJl0dWvUqlXMKpo2LV7Mp06F3XaDAw6I/v1sli2D3/wmXvS//BKefRauuw7atm262kWSpiCQVbVtG/0hkybFW+RTT41zk+t7NS0gZWXw85/HCZ9XXhn9+wMHwuGHx7hCbW++CYMGxWygior4ep99kqlbJEkKAqnbVlvFBP577olDiQcNghEjojO+CLRpA7/4BcyaBZdcAi++GAO+xx4bQyKXXx7DIR98EKuZ77gDOnZMumqRZJg39ry7PCsvL/fKysqky0iXxYtjm4rrrouJ/FdcEZP5i2hF1aefwlVXRbfR55/H9446Knq+unVLtjaRpmBm49y9vM5rCgLJ2aRJ0VX06qvRqX7DDTGRv4gsXAg33wx9+8LRR2uPIEmP+oKgeN7SSfL694eXXooJ9tOmxa5rZ50FixYlXVnOunWLBdbHHKMQEKmmIJC1U1IS3ULvvBPTdK69NvZcGDUq//s9iEheKAikYTp3jq6hsWNjR7jjj4+tru++u+BXJ4vIqhQEsm7Ky2HMmJhdVFIS8zD79YsN1At8d1MRCQoCWXctWsQOcBMnxlzMLl3g5JNjxfKf/xx7N4hIwVIQSOMpKYmVW2PHxmloPXvCGWdA797whz/AkiVJVygidVAQSOMzg/33j13bXnwxZhudd15sgX3JJfDZZ0lXKCK1KAgkv3bfHZ55Bv7zn9ga9MILYdNNYw7nwoVJVyciKAikqQwaBI89Ftt6Hnhg7PHQqxecc07s8yAiiVEQSNP63vfgvvviyMyjj451CL17w89+BrNnJ12dSCopCCQZW24Zp9C/+26cfXDbbbHvw4knxvdEpMkoCCRZm20WJ8rMmAGnnRathS23jOPH3nwz6epEUkFBIIWhR484M3n27DgY5x//iNlGRxxRNGchiBQrBYEUlvXXh9//HubMgYsuik3uBg6Mo8Zefjnp6kSapbwFgZndZmYLzGxylutmZtea2XQzm2RmO+SrFilCXbrEGQhz5sT5B+PHx1TUPfaI8ySLbPt0kUKWzxbBHcAB9Vw/EOibuQ0H/pLHWqRYtW8fi9FmzYpTZWbMgP32i+MzH3tMgSDSCPIWBO7+ElDfuYaHAyM9/AfoZGbd81WPFLk2beIw4hkz4K9/jcVohx8e50/+7W+wcmXSFYoUrSTHCDYG3qv19bzM977FzIabWaWZVS7UatR0a90ahg+PKaYjR8YOp8ceC9/9bny9fHnSFYoUnaIYLHb3m9y93N3Lu+mAWQEoLY0zECZPhvvvh7Iy+NGPYIstosXw9ddJVyhSNJIMgveBTWp93SPzPZHctWgRK5THj4e//x022ABGjIj1CddcA19+mXSFIgUvySB4DDghM3vo+8Aid5+fYD1SzMzgkEPikJznnouWwVlnxX5Gl18OixcnXaFIwcrn9NHRwBign5nNM7OTzGyEmY3I3OUJYCYwHbgZODVftUiKmMHee8O//hXrDgYMgAsuiLMRLroIPqlv/oJIOpkX2fS78vJyr6ysTLoMKSbjxsFll8HDD0O7dnDqqTGe0K9fdC2JpICZjXP38jqvKQgkNSZPht/9LvYzqqqCtm1h++2j1TBgQJy/vMUWCgdplhQEIrXNnh1bV1RWRmth/Piac5Wrw6G8vCYgFA7SDCgIROqzYgW8/XaEQnU4TJhQEw7t2tUdDiVFMftaBFAQiKy9FStg6tQIheqAmDABvvoqrrdvX9OtVB0QffsqHKRgKQhEGsOKFXGyWu1wmDhx1XDYYYdVw2HzzRUOUhAUBCL5snx5tByqu5Squ5WqVzZ36PDtcOjTR+EgTU5BINKUli+PlkPtcJg4cdVwqB5rqA6IPn1iDYRInigIRJK2fDlMmbLqgPTEibFpHkDHjtFyqG41lJfHNhkKB2kkCgKRQrRsWU04VAfEpEk14dCpU004DBwYtx49FA7SIAoCkWKxbFksfFs9HKq31+7evSYUBg2KkOjYMdmapSgoCESK2ddfRzfS2LHw2mvx8d13a65vuWVNMAwcCP37Q6tWydUrBUlBINLcfPpptBaqg+G112DBgrjWunWscajdctBgdOopCESaO3eYOzdCoToYxo2rOY+hc+dVg2HgQNAhT6miIBBJo+oFcLW7lCZPjg33IM5qqA6FgQNjYLpNm0RLlvxREIhI+OKLaClUtxzGjoU5c+Jaixaw7barthy22kob7jUTCgIRye7DD+H111cNh88+i2tt29ZMX61uPWgKa1FSEIhI7qqqYPr0VbuUJkyoWd+w4YardimVl8eaBylo9QVBaVMXIyIFrqQkttneYguoqDs18sYAAAi9SURBVIjv1Z7CWn179NGax1RPYR04ELbeGnr3jpZDqV5iioH+lURkzVq3rnmhr1Y9hbW65fDUUzByZM31Fi1gk01iULp3729/7N5d4w8FQkEgIg3TuTPsu2/cIKawvvceTJsWp8DNmlXz8emn4YMPVn18y5bQs+eqAVH78w031FhEE1EQiEjjMINNN41bXb76KtY61A6I6o+PPlqzIK5aWVkERV2tiV69oGtXBUUjURCISNMoK6sZe6jLF1/EVNbVg2L27Oh++uSTVe/ftm321kTv3jGAraDIiYJARApD27Yx0Lz11nVfX7y4JhhWD4uXXorrtXXokL010bt3nCgngIJARIpFhw6xoV7//t++5h5rH+rqdpo+HZ59tma7jWpdukQg9OlTc9t88/i40UapOkVOQSAixc8sBq87d46tMlbnDh99tGp306xZMHNmrLR+6KHYkqNaWVmERHUw1A6Knj2b3e6uCgIRaf7MYpO9bt1WnQJbbcWKGMieMSNu06fXfP7886u2JkpKYkB89VZE9a1du6b7czUSBYGISGlpHA262WY102Grucc2HNXBUDssHnwQPv541fuvv37dLYk+fQp2ppOCQESkPmax+K17d9h1129fX7So7pbECy/AqFERJNXat6+7FdGnT6zETmiBnYJARGRddOwY4xJ1jU189VWMRazekpg0KdZOVB9BCjHuUHvwunZY9O4dq7vzREEgIpIvZWWxlfdWW3372sqVMG/et1sSM2bAyy/DkiU19zWLFsOZZ8K55zZ6mQoCEZEktGgRM5B69oTBg1e95g4LF357XKJ797yUoiAQESk0ZjHovP76sNNOef9x6VkxISIidVIQiIiknIJARCTlFAQiIimX1yAwswPM7B0zm25m/1vH9U3N7F9mNt7MJpnZQfmsR0REvi1vQWBmLYDrgQOBrYGhZrb6/rK/Bv7m7tsDQ4Ab8lWPiIjULZ8tgoHAdHef6e7LgHuBw1e7jwMdMp93BFY7y05ERPItn0GwMfBera/nZb5X20VAhZnNA54AzqjricxsuJlVmlnlwoUL81GriEhqJb2gbChwh7tfZWY7AXeZ2TbuXlX7Tu5+E3ATgJktNLM5Dfx5XYGP1qniplVM9RZTrVBc9RZTrVBc9RZTrbBu9fbMdiGfQfA+sEmtr3tkvlfbScABAO4+xszKiD/oaqdY13D3bg0tyMwq3b28oY9vasVUbzHVCsVVbzHVCsVVbzHVCvmrN59dQ68Dfc2st5m1IgaDH1vtPnOBvQHMbCugDFDfj4hIE8pbELj7CuB04GlgKjE7aIqZXWxmh2Xudi5wsplNBEYDP3avvXm3iIjkW17HCNz9CWIQuPb3Lqz1+VvALvmsYTU3NeHPagzFVG8x1QrFVW8x1QrFVW8x1Qp5qtf0BlxEJN20xYSISMopCEREUi41QbCmfY8KiZndZmYLzGxy0rWsiZltktkv6i0zm2JmZyZdUzZmVmZmY81sYqbW3yZdUy7MrEVmP67Hk66lPmY228zeNLMJZlaZdD1rYmadzOwBM3vbzKZm1jIVHDPrl/k7rb4tNrOzGvVnpGGMILPv0bvAvsQK59eBoZnB6oJjZrsDnwMj3X2bpOupj5l1B7q7+xtm1h4YBxxRiH+3ZmZAW3f/3MxaAq8AZ7r7fxIurV5mdg5QDnRw90OSricbM5sNlLt7USzQMrM7gZfd/ZbMFPc27v5Z0nXVJ/Na9j4wyN0burD2W9LSIshl36OC4e4vAZ8kXUcu3H2+u7+R+XwJMVV49a1ECoKHzzNftszcCvqdkJn1AA4Gbkm6lubEzDoCuwO3Arj7skIPgYy9gRmNGQKQniDIZd8jWUdm1gvYHngt2Uqyy3SzTCBWrz/r7gVba8bVwHlA1ZruWAAceMbMxpnZ8KSLWYPexOLV2zPdbreYWduki8rBEGLNVaNKSxBInplZO+BB4Cx3X5x0Pdm4+0p3347Y8mSgmRVs15uZHQIscPdxSdeSo13dfQdi6/nTMl2chaoU2AH4S2Yb/C+AQh87bAUcBtzf2M+dliDIZd8jaaBMf/uDwN3u/lDS9eQi0w3wLzJ7XRWoXYDDMn3v9wKDzWxUsiVl5+7vZz4uAB4mumQL1TxgXq0W4QNEMBSyA4E33P2/jf3EaQmCXPY9kgbIDMDeCkx19z8mXU99zKybmXXKfL4eMXng7WSrys7dL3D3Hu7ei/id/ae7VyRcVp3MrG1msgCZLpb9gIKd9ebuHwLvmVm/zLf2BgpugsNqhpKHbiFIfhvqJuHuK8yset+jFsBt7j4l4bKyMrPRwJ5A18xZDf/n7rcmW1VWuwDHA29m+t4BfpnZXqTQdAfuzMy8KCH2vyroKZlFZAPg4XhfQClwj7s/lWxJa3QGcHfmzeFM4MSE68kqE677Aqfk5fnTMH1URESyS0vXkIiIZKEgEBFJOQWBiEjKKQhERFJOQSAiknIKApHVmNnK1XZ7bLQVp2bWqxh2lZV0ScU6ApG1tDSzDYVIKqhFIJKjzH77V2b23B9rZptnvt/LzP5pZpPM7Hkz2zTz/Q3M7OHM+QcTzWznzFO1MLObM2ciPJNZ5SySGAWByLett1rX0LG1ri1y922BPxM7gwJcB9zp7v2Bu4FrM9+/FnjR3b9H7GNTvZq9L3C9u38X+Aw4Ks9/HpF6aWWxyGrM7HN3b1fH92cDg919ZmajvQ/d/Ttm9hFxOM/yzPfnu3tXM1sI9HD3r2s9Ry9i++u+ma/PB1q6+6X5/5OJ1E0tApG141k+Xxtf1/p8JRqrk4QpCETWzrG1Po7JfP5vYndQgOOAlzOfPw/8DL45EKdjUxUpsjb0TkTk29artZMqwFPuXj2FtLOZTSLe1Q/NfO8M4qSrXxCnXlXvYnkmcJOZnUS88/8ZMD/v1YusJY0RiOSo2A5nF8mVuoZERFJOLQIRkZRTi0BEJOUUBCIiKacgEBFJOQWBiEjKKQhERFLu/wOvUnKSFy7SuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log = pd.read_csv(f'{LOG_DIR}/log_{MODEL_NAME}_{VER}.csv')\n",
    "sns.lineplot(x=log['Epoch'], y=log['Valid Loss'], color='blue')\n",
    "sns.lineplot(x=log['Epoch'], y=log['Train Loss'], color='red')\n",
    "#sns.lineplot(x=log['Epoch'], y=log['Recall'], color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model --- Stage 2\n",
    "\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "save_path = f'{MDL_DIR}/{MODEL_NAME}_{VER}/{MODEL_NAME}_'+str(best_epoch)+'.pth'\n",
    "load_weights = torch.load(save_path)\n",
    "model_ft.load_state_dict(load_weights)\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.7, patience=2, min_lr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log = pd.read_csv(f'{LOG_DIR}/log_{MODEL_NAME}_{VER}.csv')\n",
    "sns.lineplot(x=log['Epoch'], y=log['Valid Loss'], color='blue')\n",
    "sns.lineplot(x=log['Epoch'], y=log['Train Loss'], color='red')\n",
    "#sns.lineplot(x=log['Epoch'], y=log['Recall'], color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/36\n",
      "----------\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af633523c6404cb7bb724050423f81dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.8455 Acc: 0.8635\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b910a94051401b849864dc3e67ebbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.7727272727272727, vowel 0.8083333333333333, consonant 0.9846153846153847, total 0.8346008158508158\n",
      "valid recall score is 0.835\n",
      "val Loss: 1.2703 Acc: 0.8411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryoooo1114/anaconda3/envs/analysis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/36\n",
      "----------\n",
      "1952\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff18b65a88c4c98adbc62d6ae71b21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.8054 Acc: 0.8668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f16d25f24e84f20b7ac81052c5263e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.8571428571428571, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.883058608058608\n",
      "valid recall score is 0.883\n",
      "val Loss: 1.2051 Acc: 0.8643\n",
      "Epoch 9/36\n",
      "----------\n",
      "3883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddd088eb1b7444195e86a34574edb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.7887 Acc: 0.8677\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14ca65e4d714f4b8843de3b98849ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.8571428571428571, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.883058608058608\n",
      "valid recall score is 0.883\n",
      "val Loss: 1.0738 Acc: 0.8571\n",
      "Epoch 10/36\n",
      "----------\n",
      "5790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6931d8bca804af791730451cf182eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.7486 Acc: 0.8688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a9e3444c5a4f6d8a29a2e3203167d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.7045454545454546, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.8067599067599067\n",
      "valid recall score is 0.807\n",
      "val Loss: 1.2059 Acc: 0.8452\n",
      "Early stopping counter: 1\n",
      "Epoch 11/36\n",
      "----------\n",
      "7672\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6f7c0805f44f628ec5998a97ed2582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.7509 Acc: 0.8747\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41c4634d60b4888a40cabc556e663f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.1, vowel 0.25416666666666665, consonant 0.38717948717948714, total 0.21033653846153844\n",
      "valid recall score is 0.210\n",
      "val Loss: 1.2145 Acc: 0.8582\n",
      "Early stopping counter: 2\n",
      "Epoch 12/36\n",
      "----------\n",
      "9559\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289e0de5495948649b09cfa50a8c8271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.7640 Acc: 0.8716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99b2b582f89470fbe41dd341e66173d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.8571428571428571, vowel 0.8333333333333333, consonant 1.0, total 0.8869047619047619\n",
      "valid recall score is 0.887\n",
      "val Loss: 1.0496 Acc: 0.8723\n",
      "Epoch 13/36\n",
      "----------\n",
      "11488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494b37a5b67747c7b9212c38127b7b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.7295 Acc: 0.8703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecbdbb779be43ce81a147a415333a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.7727272727272727, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.8408508158508159\n",
      "valid recall score is 0.841\n",
      "val Loss: 1.2765 Acc: 0.8542\n",
      "Early stopping counter: 3\n",
      "Epoch 14/36\n",
      "----------\n",
      "13396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b4964b7cf04545b6b8815568e9647a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.6722 Acc: 0.8790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20bff971d404b26a74bc9c7e514bd48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.8571428571428571, vowel 1.0, consonant 1.0, total 0.9285714285714286\n",
      "valid recall score is 0.929\n",
      "val Loss: 1.1303 Acc: 0.8525\n",
      "Early stopping counter: 4\n",
      "Epoch 15/36\n",
      "----------\n",
      "15179\n",
      "Early Stopped since loss have not decreased for 4 epoch.\n",
      "Training complete in 413m 35s\n",
      "Best val Acc: 0.872336\n"
     ]
    }
   ],
   "source": [
    "#alpha 1 \n",
    "model_ft, best_epoch = train_model(model_ft, dataloaders, criterion, optimizer, scheduler,best_epoch, NUM_EPOCH, DEVICE, PATIAENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(DEVICE)\n",
    "best_epoch = 11\n",
    "save_path = f'{MDL_DIR}/{MODEL_NAME}_{VER}/{MODEL_NAME}_'+str(best_epoch)+'.pth'\n",
    "load_weights = torch.load(save_path)\n",
    "model_ft.load_state_dict(load_weights)\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=0.0005)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.7, patience=2, min_lr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/36\n",
      "----------\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f73d6b1de54e6e846624d28021476a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.3294 Acc: 0.8937\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbecb25a46214580879ebf1fd4c4e708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.7727272727272727, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.8408508158508159\n",
      "valid recall score is 0.841\n",
      "val Loss: 0.4364 Acc: 0.8745\n",
      "Epoch 13/36\n",
      "----------\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e989f2ac1144729ea9080b365ab1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.3094 Acc: 0.8918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7366d0390c7e40dba55a1a398b070f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.7727272727272727, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.8408508158508159\n",
      "valid recall score is 0.841\n",
      "val Loss: 0.4310 Acc: 0.8985\n",
      "Epoch 14/36\n",
      "----------\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f4e57ba835446685e2d1d313838789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2919 Acc: 0.8930\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41099b40732b47c0a701e43d21c68da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.7045454545454546, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.8067599067599067\n",
      "valid recall score is 0.807\n",
      "val Loss: 0.4332 Acc: 0.8943\n",
      "Early stopping counter: 1\n",
      "Epoch 15/36\n",
      "----------\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a613c0c6d0de4d79bc88c6f7b150f6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2656 Acc: 0.8997\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5327229cc4b415c9516ea389a7691db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.7727272727272727, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.8408508158508159\n",
      "valid recall score is 0.841\n",
      "val Loss: 0.4634 Acc: 0.8914\n",
      "Early stopping counter: 2\n",
      "Epoch 16/36\n",
      "----------\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f96ffe7eba74280809891d63049294a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2646 Acc: 0.9017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1372eb5692c642bb8e66a2067e371811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.7727272727272727, vowel 0.8333333333333333, consonant 1.0, total 0.8446969696969697\n",
      "valid recall score is 0.845\n",
      "val Loss: 0.4160 Acc: 0.8873\n",
      "Epoch 17/36\n",
      "----------\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ed8f9468bf4479a9eb3f7f66838ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2532 Acc: 0.9034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560785f320f640088345cccec789c4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.95, vowel 0.85, consonant 1.0, total 0.9375\n",
      "valid recall score is 0.938\n",
      "val Loss: 0.3718 Acc: 0.9036\n",
      "Epoch 18/36\n",
      "----------\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c894b6db27f848589c1e02bf394a3ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2225 Acc: 0.9078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc22b53aff940739ba31271158d809e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.6956521739130435, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.8023132664437012\n",
      "valid recall score is 0.802\n",
      "val Loss: 0.3763 Acc: 0.8762\n",
      "Early stopping counter: 3\n",
      "Epoch 19/36\n",
      "----------\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9811651c11e9409ab86e351679f01db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2296 Acc: 0.9099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16c35015e074f0e8ba9f3fcc7624539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall: grapheme 0.8571428571428571, vowel 0.8333333333333333, consonant 0.9846153846153847, total 0.883058608058608\n",
      "valid recall score is 0.883\n",
      "val Loss: 0.3949 Acc: 0.8992\n",
      "Early stopping counter: 4\n",
      "Epoch 20/36\n",
      "----------\n",
      "0\n",
      "Early Stopped since loss have not decreased for 4 epoch.\n",
      "Training complete in 413m 32s\n",
      "Best val Acc: 0.903605\n"
     ]
    }
   ],
   "source": [
    "#alpha 1 ==> 0.1\n",
    "model_ft, best_epoch = train_model(model_ft, dataloaders, criterion, optimizer, scheduler,best_epoch, NUM_EPOCH, DEVICE, PATIAENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloaders, phase, device):\n",
    "    model.eval()\n",
    "    output_list = []\n",
    "    label_list = []\n",
    "    with torch.no_grad():\n",
    "        if phase == 'test':\n",
    "            for i, inputs in enumerate(tqdm(dataloaders)):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, pred0 = torch.max(outputs[0], 1)\n",
    "                _, pred1 = torch.max(outputs[1], 1)\n",
    "                _, pred2 = torch.max(outputs[2], 1)\n",
    "                preds = (pred0, pred1, pred2)\n",
    "                output_list.append(preds)\n",
    "            return output_list\n",
    "        elif phase == 'val':\n",
    "            for i, (inputs, labels) in enumerate(tqdm(dataloaders)):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, pred0 = torch.max(outputs[0], 1)\n",
    "                _, pred1 = torch.max(outputs[1], 1)\n",
    "                _, pred2 = torch.max(outputs[2], 1)\n",
    "                preds = (pred0, pred1, pred2)\n",
    "                output_list.append(preds)\n",
    "                label_list.append(labels.transpose(1,0))\n",
    "            return output_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = f'{MDL_DIR}/{MODEL_NAME}_{VER}/{MODEL_NAME}_'+str(best_epoch)+'.pth'\n",
    "load_weights = torch.load(save_path)\n",
    "model_ft.load_state_dict(load_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_dataset 20084\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfacd223185a49478e6230c1c0afbd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Prediction ---\n",
    "data_type = 'val'\n",
    "valid_preds_list = []\n",
    "print('valid_dataset', len(valid_dataset))\n",
    "valid_preds_list, valid_label_list = predict(model_ft, valid_loader, data_type, DEVICE)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0 (20084,) p1 (20084,) p2 (20084,)\n",
      "a0 (20084,) a1 (20084,) a2 (20084,)\n",
      "recall: grapheme 0.9645096549282974, vowel 0.9897832940731021, consonant 0.978486382031568, total 0.9743222464903162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9743222464903162"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each test_preds indicates the prediction outputs of different batch\n",
    "p0 = np.concatenate([valid_preds[0].cpu().numpy() for valid_preds in valid_preds_list], axis=0)\n",
    "p1 = np.concatenate([valid_preds[1].cpu().numpy() for valid_preds in valid_preds_list], axis=0)\n",
    "p2 = np.concatenate([valid_preds[2].cpu().numpy() for valid_preds in valid_preds_list], axis=0)\n",
    "print('p0', p0.shape, 'p1', p1.shape, 'p2', p2.shape)\n",
    "\n",
    "a0 = np.concatenate([valid_label[0].cpu().numpy() for valid_label in valid_label_list], axis=0)\n",
    "a1 = np.concatenate([valid_label[1].cpu().numpy() for valid_label in valid_label_list], axis=0)\n",
    "a2 = np.concatenate([valid_label[2].cpu().numpy() for valid_label in valid_label_list], axis=0)\n",
    "print('a0', a0.shape, 'a1', a1.shape, 'a2', a2.shape)\n",
    "\n",
    "pred_labels = [p0, p1, p2]\n",
    "y = [a0, a1, a2]\n",
    "macro_recall(pred_labels, y, n_grapheme=168, n_vowel=11, n_consonant=7)\n",
    "#fold 1 Stage-1 CV :0.9659313780016296 --> Stage-2 0.9666139805270331\n",
    "#fold 2 Stage-1 CV :0.9698549400969805 --> Stage 2 0.9743222464903162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prediction ---\n",
    "data_type = 'test'\n",
    "test_preds_list = []\n",
    "for i in range(4):\n",
    "    # --- prepare data ---\n",
    "    indices = [i]\n",
    "    test_images = prepare_image(\n",
    "        DATADIR, FEATHERDIR, data_type = data_type, submission=True, indices=indices)\n",
    "    n_dataset = len(test_images)\n",
    "    print(f'i={i}, n_dataset={n_dataset}')\n",
    "    # test_data_size = 200 if debug else int(n_dataset * 0.9)\n",
    "    test_dataset = BengaliAIDataset(\n",
    "    test_images, None,\n",
    "    transform=data_transforms[data_type])\n",
    "    print('test_dataset', len(test_dataset))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKER)\n",
    "    \n",
    "    test_preds_list = predict(model_ft, test_loader, data_type,DEVICE)\n",
    "    del test_images\n",
    "    gc.collect()\n",
    "    if DEBUG:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each test_preds indicates the prediction outputs of different batch\n",
    "p0 = np.concatenate([test_preds[0].cpu().numpy() for test_preds in test_preds_list], axis=0)\n",
    "p1 = np.concatenate([test_preds[1].cpu().numpy() for test_preds in test_preds_list], axis=0)\n",
    "p2 = np.concatenate([test_preds[2].cpu().numpy() for test_preds in test_preds_list], axis=0)\n",
    "print('p0', p0.shape, 'p1', p1.shape, 'p2', p2.shape)\n",
    "\n",
    "row_id = []\n",
    "target = []\n",
    "for i in tqdm(range(len(p0))):\n",
    "    row_id += [f'Test_{i}_grapheme_root', f'Test_{i}_vowel_diacritic',\n",
    "               f'Test_{i}_consonant_diacritic']\n",
    "    target += [p0[i], p1[i], p2[i]]\n",
    "pred_df = pd.DataFrame({'row_id': row_id, 'target': target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
