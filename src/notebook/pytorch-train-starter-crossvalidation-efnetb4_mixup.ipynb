{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../input/train.csv\n",
      "../../input/test.csv\n",
      "../../input/class_map.csv\n",
      "../../input/.gitkeep\n",
      "../../input/sample_submission.csv\n",
      "../../input/parquet/train_image_data_2.parquet\n",
      "../../input/parquet/test_image_data_2.parquet\n",
      "../../input/parquet/train_image_data_3.parquet\n",
      "../../input/parquet/test_image_data_1.parquet\n",
      "../../input/parquet/test_image_data_0.parquet\n",
      "../../input/parquet/test_image_data_3.parquet\n",
      "../../input/parquet/train_image_data_0.parquet\n",
      "../../input/parquet/train_image_data_1.parquet\n",
      "../../input/feather/train_image_data_2.feather\n",
      "../../input/feather/test_image_data_1.feather\n",
      "../../input/feather/train_image_data_1.feather\n",
      "../../input/feather/test_image_data_3.feather\n",
      "../../input/feather/train_image_data_0.feather\n",
      "../../input/feather/test_image_data_0.feather\n",
      "../../input/feather/test_image_data_2.feather\n",
      "../../input/feather/train_image_data_3.feather\n"
     ]
    }
   ],
   "source": [
    "#+---- Basic Libraries ----+#\n",
    "import sys, os, time, gc, random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from utils import *\n",
    "\n",
    "#+---- Utilities Libraries ----+#\n",
    "#import albumentations as albu\n",
    "import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#+---- Pytorch Libraries ----+#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils import model_zoo\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#+---- List the input data ----+#\n",
    "for dirname, _, filenames in os.walk('../../input/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Initial Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path('../../input')\n",
    "FEATHERDIR = Path('../../input/feather')\n",
    "OUTDIR = Path('../../output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input', '.gitignore', 'README.md', '.git', 'src', 'submission', 'output']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG =False\n",
    "SUBMISSION =False\n",
    "BATCH_SIZE =64\n",
    "NUM_EPOCH = 36\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "MDL_DIR = '../models'\n",
    "LOG_DIR = '../logs'\n",
    "IMAGE_SIZE=128\n",
    "TRAIN_RATIO = 0.9\n",
    "WORKER = 4\n",
    "SEED = 6666\n",
    "MODEL_NAME ='efficientnet-b4'\n",
    "N_Fold = 10\n",
    "CV = True\n",
    "Fold = 1\n",
    "PATIAENCE = 4\n",
    "VER = 'fold_1_mixup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grapheme = 168\n",
    "n_vowel = 11\n",
    "n_consonant = 7\n",
    "n_total = n_grapheme + n_vowel + n_consonant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform class for data preprocessing and augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(datadir, featherdir, data_type='train',\n",
    "                  submission=False, indices=[0, 1, 2, 3]):\n",
    "    assert data_type in ['train', 'test']\n",
    "    if submission:\n",
    "        image_df_list = [pd.read_parquet(datadir / f'{data_type}_image_data_{i}.parquet')\n",
    "                         for i in indices]\n",
    "    else:\n",
    "        image_df_list = [pd.read_feather(featherdir / f'{data_type}_image_data_{i}.feather')\n",
    "                         for i in indices]\n",
    "\n",
    "    print('image_df_list', len(image_df_list))\n",
    "    HEIGHT = 137\n",
    "    WIDTH = 236\n",
    "    images = [df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH) for df in image_df_list]\n",
    "    del image_df_list\n",
    "    gc.collect()\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_char_image(image, threshold=5./255.):\n",
    "    assert image.ndim == 2\n",
    "    is_black = image > threshold\n",
    "\n",
    "    is_black_vertical = np.sum(is_black, axis=0) > 0\n",
    "    is_black_horizontal = np.sum(is_black, axis=1) > 0\n",
    "    left = np.argmax(is_black_horizontal)\n",
    "    right = np.argmax(is_black_horizontal[::-1])\n",
    "    top = np.argmax(is_black_vertical)\n",
    "    bottom = np.argmax(is_black_vertical[::-1])\n",
    "    height, width = image.shape\n",
    "    cropped_image = image[left:height - right, top:width - bottom]\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.ColorJitter(0.5,0.5,0.5,0.5),\n",
    "        transforms.RandomAffine(degrees=0.6),\n",
    "        transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    \n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliAIDataset(Dataset):\n",
    "    def __init__(self, images, labels=None, transform=None, indices=None):\n",
    "        self.transform = transform\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        if indices is None:\n",
    "            indices = np.arange(len(images))\n",
    "        self.indices = indices\n",
    "        self.train = labels is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"return length of this dataset\"\"\"\n",
    "        return len(self.indices)\n",
    "      \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Return i-th data\"\"\"\n",
    "        i = self.indices[i]\n",
    "        x = self.images[i]\n",
    "        # Opposite white and black: background will be white and\n",
    "        # for future Affine transformation\n",
    "        x = (255 - x).astype(np.float32) #/ 255.\n",
    "        x = crop_char_image(x)\n",
    "        x = Image.fromarray(x).convert(\"RGB\")\n",
    "        x = self.transform(x)\n",
    "        if self.train:\n",
    "            y = self.labels[i]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "Fold 10\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATADIR/'train.csv')\n",
    "train['id'] = train['image_id'].apply(lambda x: int(x.split('_')[1]))\n",
    "X, y = train[['id', 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic']]\\\n",
    ".values[:,0], train.values[:,1:]\n",
    "train['fold'] = np.nan\n",
    "mskf = MultilabelStratifiedKFold(n_splits=N_Fold)\n",
    "for i, (_, index) in enumerate(mskf.split(X, y)):\n",
    "    print('Fold '+str(i+1))\n",
    "    train.iloc[index, -1] = i\n",
    "train['fold'] = train['fold'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_df_list 4\n",
      "CPU times: user 9.25 s, sys: 12 s, total: 21.3 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train = pd.read_csv(DATADIR/'train.csv')\n",
    "train_labels = train[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].values\n",
    "indices = [0] if DEBUG else [0, 1, 2, 3]\n",
    "train_images = prepare_image(\n",
    "    DATADIR, FEATHERDIR, data_type='train', submission=False, indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dataset = len(train_images)\n",
    "\n",
    "if not CV:\n",
    "    train_data_size = 200 if DEBUG else int(n_dataset * TRAIN_RATIO)\n",
    "    valid_data_size = 100 if DEBUG else int(n_dataset - train_data_size)\n",
    "    perm = np.random.RandomState(777).permutation(n_dataset)\n",
    "    print('perm', perm)\n",
    "\n",
    "    train_dataset = BengaliAIDataset(\n",
    "        train_images, train_labels, transform=data_transforms['train'],\n",
    "        indices=perm[:train_data_size])\n",
    "\n",
    "    valid_dataset = BengaliAIDataset(\n",
    "        train_images, train_labels, transform=data_transforms['val'],\n",
    "        indices=perm[train_data_size:train_data_size+valid_data_size])\n",
    "else:\n",
    "    valid_idx = np.array(train[train['fold']==Fold].index)\n",
    "    trn_idx = np.array(train[train['fold']!=Fold].index)\n",
    "    trn_idx = trn_idx[:200] if DEBUG else trn_idx\n",
    "    valid_idx = valid_idx[:100] if DEBUG else valid_idx\n",
    "    \n",
    "    train_dataset = BengaliAIDataset(\n",
    "        train_images, train_labels, transform=data_transforms['train'],\n",
    "        indices=trn_idx)\n",
    "    valid_dataset = BengaliAIDataset(\n",
    "        train_images, train_labels, transform=data_transforms['val'],\n",
    "        indices=valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKER)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKER)\n",
    "\n",
    "dataloaders = {'train':train_loader, 'val': valid_loader}\n",
    "dataset_sizes = {'train':len(train_dataset), 'val': len(valid_dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image torch.Size([3, 128, 128]) label [159   0   0]\n"
     ]
    }
   ],
   "source": [
    "image, label = train_dataset[1]\n",
    "print('image', image.shape, 'label', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model/Train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohem_loss(rate, cls_pred, cls_target):\n",
    "    batch_size = cls_pred.size(0) \n",
    "    ohem_cls_loss = F.cross_entropy(cls_pred, cls_target, reduction='none', ignore_index=-1)\n",
    "\n",
    "    sorted_ohem_loss, idx = torch.sort(ohem_cls_loss, descending=True)\n",
    "    keep_num = min(sorted_ohem_loss.size()[0], int(batch_size*rate) )\n",
    "    if keep_num < sorted_ohem_loss.size()[0]:\n",
    "        keep_idx_cuda = idx[:keep_num]\n",
    "        ohem_cls_loss = ohem_cls_loss[keep_idx_cuda]\n",
    "    cls_loss = ohem_cls_loss.sum() / keep_num\n",
    "    return cls_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_recall(pred_labels, y, n_grapheme=168, n_vowel=11, n_consonant=7):\n",
    "    recall_grapheme = sklearn.metrics.recall_score(pred_labels[0], y[0], average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(y[1],pred_labels[1], average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(pred_labels[2], y[2], average='macro')\n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_score = np.average(scores, weights=[2, 1, 1])\n",
    "    print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, '\n",
    "           f'total {final_score}')\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(preds_list, label_list):\n",
    "    #preds_list is torch tensor to device\n",
    "    #label_list is torch tensor to device\n",
    "    _, pred0 = torch.max(preds_list[0], 1)\n",
    "    _, pred1 = torch.max(preds_list[1], 1)\n",
    "    _, pred2 = torch.max(preds_list[2], 1)\n",
    "    p0 = pred0.cpu().numpy()\n",
    "    p1 = pred1.cpu().numpy()\n",
    "    p2 = pred2.cpu().numpy()\n",
    "    pred_labels = [p0, p1, p2]\n",
    "    #print(pred_labels)\n",
    "    a0 = label_list[0].cpu().numpy()\n",
    "    a1 = label_list[1].cpu().numpy()\n",
    "    a2 = label_list[2].cpu().numpy() \n",
    "    y = [a0, a1, a2]\n",
    "    #print(y)\n",
    "    return pred_labels, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "def cutmix(data, targets1, targets2, targets3, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets1 = targets1[indices]\n",
    "    shuffled_targets2 = targets2[indices]\n",
    "    shuffled_targets3 = targets3[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "\n",
    "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, \\\n",
    "               shuffled_targets3, lam]\n",
    "    return data, targets\n",
    "\n",
    "def mixup(data, targets1, targets2, targets3, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets1 = targets1[indices]\n",
    "    shuffled_targets2 = targets2[indices]\n",
    "    shuffled_targets3 = targets3[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3,\\\n",
    "               shuffled_targets3, lam]\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def cutmix_criterion(preds1,preds2,preds3, targets):\n",
    "    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1],\\\n",
    "    targets[2], targets[3], targets[4], targets[5], targets[6]\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2) +\\\n",
    "lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4) +\\\n",
    "lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)\n",
    "\n",
    "def mixup_criterion(preds1,preds2,preds3, targets):\n",
    "    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], \\\n",
    "    targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2)\\\n",
    "+ lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4) +\\\n",
    "lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "code_folding": [
     151,
     160
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler,start_epoch,\n",
    "                num_epochs, device, patiance):\n",
    "    since = time.time()\n",
    "    \n",
    "    trn_loss_list =[]\n",
    "    trn_acc_list = []\n",
    "    val_loss_list =[]\n",
    "    val_acc_list = []\n",
    "    epoch_list = []\n",
    "    recall_list = []\n",
    "    mix_cnt = 0\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 10\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs)[start_epoch:]:\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)  \n",
    "        \n",
    "        if early_stopping_counter == patiance:\n",
    "            print(f'Early Stopped since loss have not decreased for {patiance} epoch.')\n",
    "            break\n",
    "        epoch_list.append(epoch+1)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            dataset_sizes = len(dataloaders[phase].dataset)\n",
    "            length = int(np.floor(dataset_sizes/BATCH_SIZE))\n",
    "            mixuplist = random.sample( range(length), int(0.2*length))\n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (inputs, labels) in enumerate(tqdm(dataloaders[phase])):\n",
    "                #print(inputs.shape)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.transpose(1,0).to(device) #use when single label for one image\n",
    "\n",
    "                grapheme_root = labels[0]\n",
    "                vowel_diacritic = labels[1]\n",
    "                consonant_diacritic = labels[2]\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):                                 \n",
    "                    if phase == 'train':\n",
    "                        if idx in mixuplist:\n",
    "                            mix_cnt += 1\n",
    "                            inputs_mixed, labels_mixed = mixup(inputs, grapheme_root, \\\n",
    "                                                               vowel_diacritic, consonant_diacritic, 0.1)\n",
    "\n",
    "                            outputs = model(inputs_mixed) \n",
    "                            grapheme_root_prd = outputs[0]\n",
    "                            vowel_diacritic_prd = outputs[1]\n",
    "                            consonant_diacritic_prd = outputs[2]\n",
    "                            loss = mixup_criterion(grapheme_root_prd,vowel_diacritic_prd,\\\n",
    "                                               consonant_diacritic_prd, labels_mixed)\n",
    "                        \n",
    "                        else:\n",
    "                            outputs = model(inputs)\n",
    "                            grapheme_root_prd = outputs[0]\n",
    "                            vowel_diacritic_prd = outputs[1]\n",
    "                            consonant_diacritic_prd = outputs[2]\n",
    "                            loss = (1/4)*(2*criterion(grapheme_root_prd, grapheme_root)+\\\n",
    "                                  criterion(vowel_diacritic_prd, vowel_diacritic) +\\\n",
    "                                     criterion(consonant_diacritic_prd, consonant_diacritic))\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    if phase == 'val':\n",
    "                        outputs = model(inputs)\n",
    "                        grapheme_root_prd = outputs[0]\n",
    "                        vowel_diacritic_prd = outputs[1]\n",
    "                        consonant_diacritic_prd = outputs[2]\n",
    "                        loss = (1/4)*(2*criterion(grapheme_root_prd, grapheme_root)+\\\n",
    "                              criterion(vowel_diacritic_prd, vowel_diacritic) +\\\n",
    "                                 criterion(consonant_diacritic_prd, consonant_diacritic))\n",
    "                        #scheduler.step()\n",
    "                        # if plateau scheduler use following\n",
    "                        \n",
    "                # statistics: inputs.size(0) is batch size\n",
    "                epoch_loss += loss.item() * inputs.size(0) # total loss for this batch\n",
    "                epoch_corrects += torch.sum(torch.max(outputs[0], 1)[1] == labels[0])+\\\n",
    "                    torch.sum(torch.max(outputs[1], 1)[1] == labels[1])+\\\n",
    "                    torch.sum(torch.max(outputs[2], 1)[1] == labels[2])\n",
    "                \n",
    "            epoch_loss = epoch_loss / dataset_sizes\n",
    "            epoch_acc = epoch_corrects.double() / (dataset_sizes*3)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                trn_loss_list.append(epoch_loss)\n",
    "                trn_acc_list.append(epoch_acc.cpu().numpy())\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if not os.path.exists(f'{MDL_DIR}/{MODEL_NAME}_{VER}'):\n",
    "                    os.mkdir(f'{MDL_DIR}/{MODEL_NAME}_{VER}')\n",
    "                save_path = f'{MDL_DIR}/{MODEL_NAME}_{VER}/{MODEL_NAME}_'+str(epoch+1)+'.pth'\n",
    "                torch.save(model_ft.state_dict(),save_path)\n",
    "                best_epoch = epoch\n",
    "            \n",
    "            if phase == 'val':\n",
    "                \n",
    "                scheduler.step(epoch_loss)\n",
    "                val_loss_list.append(epoch_loss)\n",
    "                val_acc_list.append(epoch_acc.cpu().numpy())\n",
    "                print(mix_cnt)\n",
    "                mix_cnt = 0\n",
    "                pred, lbls = get_pred(outputs, labels)\n",
    "                recall = macro_recall(pred, lbls, \\\n",
    "                                      n_grapheme=168, n_vowel=11, n_consonant=7)\n",
    "                print('valid recall score is {:.3f}'.format(recall))\n",
    "                recall_list.append(recall)\n",
    "                # Early Stopping\n",
    "                if epoch == 0 or epoch == start_epoch:\n",
    "                    best_loss = epoch_loss\n",
    "                else:\n",
    "                    if epoch_loss < best_loss:\n",
    "                        best_loss = epoch_loss\n",
    "                        early_stopping_counter = 0\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "                        print(f'Early stopping counter: {early_stopping_counter}')\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    if not os.path.exists(f'{LOG_DIR}/log_{MODEL_NAME}_{VER}.csv'):\n",
    "        log = pd.DataFrame()\n",
    "        log['Epoch'] = epoch_list\n",
    "        log['Train Loss'] = trn_loss_list\n",
    "        log['Train Acc'] = trn_acc_list\n",
    "        log['Valid Loss'] = val_loss_list\n",
    "        log['Valid Acc'] = val_acc_list\n",
    "        log['Recall'] = recall_list\n",
    "        log.to_csv(f'{LOG_DIR}/log_{MODEL_NAME}_{VER}.csv',index=False)\n",
    "    else:\n",
    "        log = pd.DataFrame()\n",
    "        log['Epoch'] = epoch_list\n",
    "        log['Train Loss'] = trn_loss_list\n",
    "        log['Train Acc'] = trn_acc_list\n",
    "        log['Valid Loss'] = val_loss_list\n",
    "        log['Valid Acc'] = val_acc_list\n",
    "        log['Recall'] = recall_list\n",
    "        log_old = pd.read_csv(f'{LOG_DIR}/log_{MODEL_NAME}_{VER}.csv')\n",
    "        LOG = pd.concat([log_old, log], axis=0)\n",
    "        LOG.reset_index(drop=True, inplace=True)\n",
    "        LOG.to_csv(f'{LOG_DIR}/log_{MODEL_NAME}_{VER}.csv',index=False)\n",
    "    return model, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class bengali_model(nn.Module):\n",
    "    def __init__(self, num_classes1, num_classes2, num_classes3):\n",
    "        super(bengali_model, self).__init__()\n",
    "        #pretrain models\n",
    "        #self.model = pretrainedmodels.__dict__[MODEL_NAME](pretrained=None)\n",
    "        #num_ftrs = self.model.last_linear.in_features\n",
    "        #self.model.last_linear = nn.Identity()\n",
    "        \n",
    "        # EfficientNet\n",
    "        self.model = EfficientNet.from_pretrained(MODEL_NAME)\n",
    "        num_ftrs = 1792\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_ftrs, num_classes1)\n",
    "        self.fc2 = nn.Linear(num_ftrs, num_classes2)\n",
    "        self.fc3 = nn.Linear(num_ftrs, num_classes3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.model(x) #pretrain models\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = self.model.extract_features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        out1 = self.fc1(x)\n",
    "        out2 = self.fc2(x)\n",
    "        out3 = self.fc3(x)\n",
    "        return out1, out2, out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method EfficientNet.extract_features of EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        48, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        12, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (16): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (17): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (18): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (19): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (20): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (21): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (22): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (23): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (24): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (25): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (26): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (27): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (28): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (29): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (30): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (31): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2688, 112, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        112, 2688, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (_fc): Linear(in_features=1792, out_features=1000, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EfficientNet.from_pretrained(MODEL_NAME).extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "# --- Model --- Stage 1\n",
    "model_ft = bengali_model(n_grapheme, n_vowel, n_consonant)\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad6726c590d481eaa821a609f89a9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4543 Acc: 0.8776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224e2c7d15f04ad0a0e33aab9e163e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "\n",
      "val Loss: 0.2596 Acc: 0.9387\n",
      "recall: grapheme 0.8333333333333334, vowel 0.9119375573921029, consonant 0.96875, total 0.8868385560146924\n",
      "valid recall score is 0.887\n",
      "Epoch 2/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4779162503c14aa7ad743aacdcff4c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.3904 Acc: 0.8840\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3f9121137b45fbbf6960002aab0fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "\n",
      "val Loss: 0.1925 Acc: 0.9526\n",
      "recall: grapheme 0.9031007751937985, vowel 0.9277777777777778, consonant 0.9882352941176471, total 0.9305536555707554\n",
      "valid recall score is 0.931\n",
      "Epoch 3/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a57260a1760442095a45d4fcf0ec1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.3424 Acc: 0.8902\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e182fd342834dcd8ad45b56cf77603a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "\n",
      "val Loss: 0.1523 Acc: 0.9652\n",
      "recall: grapheme 0.9418604651162791, vowel 0.9233766233766233, consonant 1.0, total 0.9517743884022953\n",
      "valid recall score is 0.952\n",
      "Epoch 4/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b9df8537244276869d62663fb51ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.3223 Acc: 0.8934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea715a85021404bb7ab9c37e30796a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "\n",
      "val Loss: 0.1755 Acc: 0.9604\n",
      "recall: grapheme 0.9920634920634922, vowel 0.9743801652892562, consonant 0.8949999999999999, total 0.9633767873540602\n",
      "valid recall score is 0.963\n",
      "Early stopping counter: 1\n",
      "Epoch 5/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fca69fc4af47efa744d929c51504ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.3185 Acc: 0.8933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c13867699c4150b3b4b728a1c5424e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "\n",
      "val Loss: 0.1603 Acc: 0.9653\n",
      "recall: grapheme 0.9128787878787878, vowel 0.9097402597402596, consonant 1.0, total 0.9338744588744589\n",
      "valid recall score is 0.934\n",
      "Early stopping counter: 2\n",
      "Epoch 6/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b4f4a16e954dbca8d115ecb9629f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2706 Acc: 0.9034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18aa63721604ca68e2a1f90cecbc880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "\n",
      "val Loss: 0.1488 Acc: 0.9667\n",
      "recall: grapheme 0.9015151515151515, vowel 0.8757575757575757, consonant 0.95, total 0.9071969696969697\n",
      "valid recall score is 0.907\n",
      "Epoch 7/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01d3e4d50f24550b9759bf57efadfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2539 Acc: 0.9000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8282d35a8ead43b48b324336dbe5af2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "\n",
      "val Loss: 0.1284 Acc: 0.9718\n",
      "recall: grapheme 0.9418604651162791, vowel 0.8834710743801654, consonant 0.95, total 0.9292980011531808\n",
      "valid recall score is 0.929\n",
      "Epoch 8/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4b917bdf294a63831b5ef38a842184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2467 Acc: 0.8983\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e61e12af8043779f986d9c2ac0f4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "\n",
      "val Loss: 0.1211 Acc: 0.9724\n",
      "recall: grapheme 0.9880952380952381, vowel 0.9454545454545454, consonant 1.0, total 0.9804112554112554\n",
      "valid recall score is 0.980\n",
      "Epoch 9/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d977576147e4effafe16f47b5091679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2475 Acc: 0.9016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ee14e42ce643db9caf848d26b64be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "\n",
      "val Loss: 0.1379 Acc: 0.9699\n",
      "recall: grapheme 0.9318181818181818, vowel 0.890909090909091, consonant 0.9378787878787879, total 0.9231060606060606\n",
      "valid recall score is 0.923\n",
      "Early stopping counter: 3\n",
      "Epoch 10/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0166bfdd65c34f40b2e5856e5ce6f575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2482 Acc: 0.9088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66caa29a272b446fb807900dbec9fda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "564\n",
      "\n",
      "val Loss: 0.1280 Acc: 0.9728\n",
      "recall: grapheme 0.9573643410852715, vowel 0.890909090909091, consonant 1.0, total 0.9514094432699085\n",
      "valid recall score is 0.951\n",
      "Early stopping counter: 4\n",
      "Epoch 11/36\n",
      "----------\n",
      "Early Stopped since loss have not decreased for 4 epoch.\n",
      "Training complete in 455m 54s\n",
      "Best val Acc: 0.972798\n"
     ]
    }
   ],
   "source": [
    "# fold 1 Best val Acc: 0.974806\n",
    "# fold 2 Best val Acc: 0.975171 \n",
    "# with mixup fold 1\n",
    "model_ft, best_epoch = train_model(model_ft, dataloaders, criterion, optimizer, scheduler,0, NUM_EPOCH, DEVICE, PATIAENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log = pd.read_csv(f'{LOG_DIR}/log_{MODEL_NAME}_{VER}.csv')\n",
    "sns.lineplot(x=log['Epoch'], y=log['Valid Acc'], color='blue')\n",
    "sns.lineplot(x=log['Epoch'], y=log['Train Acc'], color='red')\n",
    "#sns.lineplot(x=log['Epoch'], y=log['Recall'], color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model --- Stage 2\n",
    "\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "save_path = f'{MDL_DIR}/{MODEL_NAME}_{VER}/{MODEL_NAME}_'+str(best_epoch)+'.pth'\n",
    "load_weights = torch.load(save_path)\n",
    "model_ft.load_state_dict(load_weights)\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=0.0005)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.7, patience=2, min_lr=1e-10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d3231aa88147d98d845916d0eafc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.2037 Acc: 0.9121\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545a5f78620e404a89acb62e01355685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1140 Acc: 0.9768\n",
      "564\n",
      "recall: grapheme 0.912878787878788, vowel 0.9233766233766233, consonant 0.9939393939393939, total 0.9357683982683982\n",
      "valid recall score is 0.936\n",
      "Epoch 11/36\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryoooo1114/anaconda3/envs/analysis/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea912add86c144c998e047f4f66436e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1895 Acc: 0.9167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d601d248341540a99abb0ed7dbbaeb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1137 Acc: 0.9781\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.890909090909091, consonant 1.0, total 0.9552854122621565\n",
      "valid recall score is 0.955\n",
      "Epoch 12/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab9916689314472810f1d2d15ebe156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1776 Acc: 0.9141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82f15948396411d9fa87bf0084d337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1253 Acc: 0.9765\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.8779220779220779, consonant 0.9939393939393939, total 0.9505235075002516\n",
      "valid recall score is 0.951\n",
      "Early stopping counter: 1\n",
      "Epoch 13/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffb018ea27e4ea490d9f3a8f29e4c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1599 Acc: 0.9167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28ffc48e0054b9f852e05a74b40afb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1349 Acc: 0.9750\n",
      "564\n",
      "recall: grapheme 0.9767441860465116, vowel 0.890909090909091, consonant 1.0, total 0.9610993657505286\n",
      "valid recall score is 0.961\n",
      "Early stopping counter: 2\n",
      "Epoch 14/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4865ca8fe0b849e285356520e456e5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1781 Acc: 0.9180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a1632213054a31b59639ee53400245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1246 Acc: 0.9777\n",
      "Epoch     5: reducing learning rate of group 0 to 3.5000e-04.\n",
      "564\n",
      "recall: grapheme 0.9573643410852715, vowel 0.890909090909091, consonant 1.0, total 0.9514094432699085\n",
      "valid recall score is 0.951\n",
      "Early stopping counter: 3\n",
      "Epoch 15/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d76b7cb4174ebcb3e53c32e1c50420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1682 Acc: 0.9207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b986ffaad1942f896ad63b0915a5c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1231 Acc: 0.9781\n",
      "564\n",
      "recall: grapheme 0.9457364341085273, vowel 0.890909090909091, consonant 1.0, total 0.9455954897815364\n",
      "valid recall score is 0.946\n",
      "Early stopping counter: 4\n",
      "Epoch 16/36\n",
      "----------\n",
      "Early Stopped since loss have not decreased for 4 epoch.\n",
      "Training complete in 272m 56s\n",
      "Best val Acc: 0.978109\n"
     ]
    }
   ],
   "source": [
    "model_ft, best_epoch = train_model(model_ft, dataloaders, criterion, optimizer, scheduler,best_epoch, NUM_EPOCH, DEVICE, PATIAENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model --- Stage 3\n",
    "\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "save_path = f'{MDL_DIR}/{MODEL_NAME}_{VER}/{MODEL_NAME}_'+str(best_epoch)+'.pth'\n",
    "load_weights = torch.load(save_path)\n",
    "model_ft.load_state_dict(load_weights)\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a671b1e0ecdc401e9263e9e452a3246a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1640 Acc: 0.9173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380a85609a994b72b561f6d434ba2df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1078 Acc: 0.9818\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Epoch 17/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996272ebf9774b67a18dd3e6dc139751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1425 Acc: 0.9162\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb404b3bc9d49eb8f8fcd1858bbbec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1063 Acc: 0.9820\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Early stopping counter: 1\n",
      "Epoch 18/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b20c99afae4de7ba6349409246e509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1525 Acc: 0.9211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4593f9b8504535899d7129e24dc460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1050 Acc: 0.9820\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Early stopping counter: 2\n",
      "Epoch 19/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c82647059c14c288d6a185077a88e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1448 Acc: 0.9171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729ac21cd0ef4ade89d1e6e09b33a4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1068 Acc: 0.9820\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Early stopping counter: 3\n",
      "Epoch 20/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272c4086c4fd4c50a153d25b09ad9872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1246 Acc: 0.9108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef14e8745fe444382253a8c676f74f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1091 Acc: 0.9820\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 0.9939393939393939, total 0.9710223268627074\n",
      "valid recall score is 0.971\n",
      "Early stopping counter: 4\n",
      "Epoch 21/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a22f513ad9c409191f615fafc46edbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1522 Acc: 0.9215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8581a984f08d416fa03cfef55e9a592a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1100 Acc: 0.9820\n",
      "Epoch    16: reducing learning rate of group 0 to 3.1250e-06.\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Early stopping counter: 5\n",
      "Epoch 22/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37b02f9d69d4d5691d3697fab3cca5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1464 Acc: 0.9178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920ae7e04a7c48f4b40eb9f3b0e1e737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1071 Acc: 0.9819\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Early stopping counter: 6\n",
      "Epoch 23/36\n",
      "----------\n",
      "Early Stopped since loss have not decreased for 6 epoch.\n",
      "Training complete in 318m 16s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "PATIAENCE = 6\n",
    "model_ft, best_epoch = train_model(model_ft, dataloaders, criterion, optimizer, scheduler,best_epoch, NUM_EPOCH, DEVICE, PATIAENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model --- Stage 4\n",
    "\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "save_path = f'{MDL_DIR}/{MODEL_NAME}_{VER}/{MODEL_NAME}_'+str(best_epoch)+'.pth'\n",
    "load_weights = torch.load(save_path)\n",
    "model_ft.load_state_dict(load_weights)\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=0.000025)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190b1eaf49ee40fe8e357f0f23e51de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1683 Acc: 0.9135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0037cf4b12a0486f9b7b75e51159381e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1071 Acc: 0.9819\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Epoch 16/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5575a549214459fa334e8326adbdeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1552 Acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6715e791ad3441598bb9682a0d071dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1081 Acc: 0.9821\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9681818181818183, consonant 1.0, total 0.9746035940803384\n",
      "valid recall score is 0.975\n",
      "Early stopping counter: 1\n",
      "Epoch 17/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bc7ac7cb0d4da48e738f707e4eccfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1578 Acc: 0.9197\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7567be3c2014376b3c07138b889f5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1093 Acc: 0.9820\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Early stopping counter: 2\n",
      "Epoch 18/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e50b7ae34e8408c915b4b313070e409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1414 Acc: 0.9199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2119375c2824deb8a892a0a5ee8e8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1067 Acc: 0.9818\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Epoch 19/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0011497af54656a95198fd25862b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1674 Acc: 0.9226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf7d5cd6bb54afb9ae3f1029821a333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1093 Acc: 0.9817\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Early stopping counter: 1\n",
      "Epoch 20/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9729556ca4104618aa7729011055bf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1604 Acc: 0.9229\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b66ea4dff74bfea3578233a33907da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1093 Acc: 0.9818\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Early stopping counter: 2\n",
      "Epoch 21/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f91a9df199649f1bb4a4507582a77ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1623 Acc: 0.9239\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d48adf12aa45a781ffaec78941739b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1079 Acc: 0.9817\n",
      "Epoch     7: reducing learning rate of group 0 to 1.2500e-05.\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 0.9939393939393939, total 0.9710223268627074\n",
      "valid recall score is 0.971\n",
      "Early stopping counter: 3\n",
      "Epoch 22/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d8a6db0509445293ffc14b0c05935f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1577 Acc: 0.9213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1883ebfa1ada4fb8ab3c4724428afdf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1082 Acc: 0.9821\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Early stopping counter: 4\n",
      "Epoch 23/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639972802d694ae3b7e11bcb164eed4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1638 Acc: 0.9205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbfeba2d6e24c46ad8faeb4927f9fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1107 Acc: 0.9818\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Early stopping counter: 5\n",
      "Epoch 24/36\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b8b21d0e894541971fc786cde11bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2825.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.1666 Acc: 0.9199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1517ae610447d293d77ea456a7c143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.1109 Acc: 0.9819\n",
      "Epoch    10: reducing learning rate of group 0 to 6.2500e-06.\n",
      "564\n",
      "recall: grapheme 0.9651162790697675, vowel 0.9599173553719008, consonant 1.0, total 0.972537478377859\n",
      "valid recall score is 0.973\n",
      "Early stopping counter: 6\n",
      "Epoch 25/36\n",
      "----------\n",
      "Early Stopped since loss have not decreased for 6 epoch.\n",
      "Training complete in 455m 29s\n",
      "Best val Acc: 0.982059\n"
     ]
    }
   ],
   "source": [
    "PATIAENCE = 6\n",
    "model_ft, best_epoch = train_model(model_ft, dataloaders, criterion, optimizer, scheduler,best_epoch, NUM_EPOCH, DEVICE, PATIAENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloaders, phase, device):\n",
    "    model.eval()\n",
    "    output_list = []\n",
    "    label_list = []\n",
    "    with torch.no_grad():\n",
    "        if phase == 'test':\n",
    "            for i, inputs in enumerate(tqdm(dataloaders)):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, pred0 = torch.max(outputs[0], 1)\n",
    "                _, pred1 = torch.max(outputs[1], 1)\n",
    "                _, pred2 = torch.max(outputs[2], 1)\n",
    "                preds = (pred0, pred1, pred2)\n",
    "                output_list.append(preds)\n",
    "            return output_list\n",
    "        elif phase == 'val':\n",
    "            for i, (inputs, labels) in enumerate(tqdm(dataloaders)):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, pred0 = torch.max(outputs[0], 1)\n",
    "                _, pred1 = torch.max(outputs[1], 1)\n",
    "                _, pred2 = torch.max(outputs[2], 1)\n",
    "                preds = (pred0, pred1, pred2)\n",
    "                output_list.append(preds)\n",
    "                label_list.append(labels.transpose(1,0))\n",
    "            return output_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = f'{MDL_DIR}/{MODEL_NAME}_{VER}/{MODEL_NAME}_'+str(best_epoch+1)+'.pth'\n",
    "load_weights = torch.load(save_path)\n",
    "model_ft.load_state_dict(load_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_dataset 20084\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fc1756f9d74d6dab02513e906e5b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5430"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Prediction ---\n",
    "data_type = 'val'\n",
    "valid_preds_list = []\n",
    "print('valid_dataset', len(valid_dataset))\n",
    "valid_preds_list, valid_label_list = predict(model_ft, valid_loader, data_type, DEVICE)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0 (20084,) p1 (20084,) p2 (20084,)\n",
      "a0 (20084,) a1 (20084,) a2 (20084,)\n",
      "recall: grapheme 0.9655606691162243, vowel 0.9886849875898626, consonant 0.9802793245519252, total 0.9750214125935591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9750214125935591"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each test_preds indicates the prediction outputs of different batch\n",
    "p0 = np.concatenate([valid_preds[0].cpu().numpy() for valid_preds in valid_preds_list], axis=0)\n",
    "p1 = np.concatenate([valid_preds[1].cpu().numpy() for valid_preds in valid_preds_list], axis=0)\n",
    "p2 = np.concatenate([valid_preds[2].cpu().numpy() for valid_preds in valid_preds_list], axis=0)\n",
    "print('p0', p0.shape, 'p1', p1.shape, 'p2', p2.shape)\n",
    "\n",
    "a0 = np.concatenate([valid_label[0].cpu().numpy() for valid_label in valid_label_list], axis=0)\n",
    "a1 = np.concatenate([valid_label[1].cpu().numpy() for valid_label in valid_label_list], axis=0)\n",
    "a2 = np.concatenate([valid_label[2].cpu().numpy() for valid_label in valid_label_list], axis=0)\n",
    "print('a0', a0.shape, 'a1', a1.shape, 'a2', a2.shape)\n",
    "\n",
    "pred_labels = [p0, p1, p2]\n",
    "y = [a0, a1, a2]\n",
    "macro_recall(pred_labels, y, n_grapheme=168, n_vowel=11, n_consonant=7)\n",
    "#fold 1 Stage-1 CV :0.9659313780016296 --> Stage-2 0.9666139805270331\n",
    "#fold 2 Stage-1 CV :0.9698549400969805 --> Stage 2 0.9743222464903162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prediction ---\n",
    "data_type = 'test'\n",
    "test_preds_list = []\n",
    "for i in range(4):\n",
    "    # --- prepare data ---\n",
    "    indices = [i]\n",
    "    test_images = prepare_image(\n",
    "        DATADIR, FEATHERDIR, data_type = data_type, submission=True, indices=indices)\n",
    "    n_dataset = len(test_images)\n",
    "    print(f'i={i}, n_dataset={n_dataset}')\n",
    "    # test_data_size = 200 if debug else int(n_dataset * 0.9)\n",
    "    test_dataset = BengaliAIDataset(\n",
    "    test_images, None,\n",
    "    transform=data_transforms[data_type])\n",
    "    print('test_dataset', len(test_dataset))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKER)\n",
    "    \n",
    "    test_preds_list = predict(model_ft, test_loader, data_type,DEVICE)\n",
    "    del test_images\n",
    "    gc.collect()\n",
    "    if DEBUG:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each test_preds indicates the prediction outputs of different batch\n",
    "p0 = np.concatenate([test_preds[0].cpu().numpy() for test_preds in test_preds_list], axis=0)\n",
    "p1 = np.concatenate([test_preds[1].cpu().numpy() for test_preds in test_preds_list], axis=0)\n",
    "p2 = np.concatenate([test_preds[2].cpu().numpy() for test_preds in test_preds_list], axis=0)\n",
    "print('p0', p0.shape, 'p1', p1.shape, 'p2', p2.shape)\n",
    "\n",
    "row_id = []\n",
    "target = []\n",
    "for i in tqdm(range(len(p0))):\n",
    "    row_id += [f'Test_{i}_grapheme_root', f'Test_{i}_vowel_diacritic',\n",
    "               f'Test_{i}_consonant_diacritic']\n",
    "    target += [p0[i], p1[i], p2[i]]\n",
    "pred_df = pd.DataFrame({'row_id': row_id, 'target': target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "201.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
